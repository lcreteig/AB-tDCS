---
title: AB_tDCS-EEG
author: Leon Reteig
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    toc: true
    toc_depth: 3
  rmdformats::html_clean:
    highlight: pygments
    gallery: true
---

# Setup environment

```{r setup}

# Load packages
library(tidyverse)  # importing, transforming, and visualizing data frames
library(here) # (relative) file paths
library(knitr) # R notebook output
library(scales) # Percentage axis labels 
library(broom) # transform model outputs into data frames
library(afex) # analysis of factorial experiments (repeated measures anova)
library(emmeans); afex_options(emmeans_model = "multivariate") # post-hoc tests / contrasts, based on multivariate model
library(ggm) # partial correlations
library(BayesFactor) # Bayesian statistics
library(metafor) # meta-analysis
library(predictionInterval) # prediction intervals (for correlations)
library(TOSTER) # equivalence tests
library(pwr) # power analysis
# Print version info
print(sessionInfo())
# Source functions
source(here("src", "func", "behavioral_analysis.R")) # loading data and calculating measures
knitr::read_chunk(here("src", "func", "behavioral_analysis.R")) # display code from file in this notebook
source(here("src", "lib", "corr_change_baseline.R")) # variance test / baseline vs. change-from-baseline correlation 
source(here("src", "lib", "appendixCodeFunctionsJeffreys.R")) # replication Bayes factors
```

- Match interpretations with statistics
- Match with outline Rmd

# Load task data

## Study 2

The following participants are excluded from further analysis at this point, because of incomplete data:

* `S03`, `S14`, `S29`, `S38`, `S43`, `S46`: their T1 performance in session 1 was less than 63% correct, so they were not invited back. This cutoff was determined based on a separate pilot study with 10 participants. It is two standard deviations below the mean of that sample.
* `S25` has no data for session 2, as they stopped responding to requests for scheduling the session
* `S31` was excluded as a precaution after session 1, as they developed a severe headache and we could not rule out the possibility this was related to the tDCS

```{r load study 2 data}
dataDir_study2 <- here("data") # root folder with AB task data
subs_incomplete <- c("S03", "S14", "S25", "S29", "S31", "S38", "S43", "S46") # don't try to load data from these participants
df_study2 <- load_data_study2(dataDir_study2, subs_incomplete) %>%
  filter(complete.cases(.)) %>% # discard rows with data from incomplete subjects
   # recode first.session ("anodal" or "cathodal") to session.order ("anodal first", "cathodal first")
  mutate(first.session = parse_factor(paste(first.session, "first"), 
                                      levels = c("anodal first", "cathodal first"))) %>%
  rename(session.order = first.session)
n_study2 <- n_distinct(df_study2$subject) # number of subjects in study 2
```

```{r print slice study 2 data}
kable(head(df_study2,13), digits = 1, caption = "Data frame for study 2")
```

The data has the following columns:

* __subject__: Participant ID, e.g. `S01`, `S12`
* __session.order__: Whether participant received `anodal` or `cathodal` tDCS in the first session (`anodal_first` vs. `cathodal_first`).
* __stimulation__: Whether participant received `anodal` or `cathodal` tDCS
* __block__: Whether data is before (`pre), during (`tDCS`) or after (`post`) tDCS
* __lag__: Whether T2 followed T1 after two distractors (lag `3`) or after 7 distractors (lag `8`)
* __trials__: Number of trials per lag that the participant completed in this block
* __T1__: Proportion of trials (out of `trials`) in which T1 was identified correctly
* __T2__: Proportion of trials (out of `trials`) in which T2 was identified correctly
* __T2.given.T1__: Proportion of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).

The number of trials vary from person-to-person, as some completed more trials in a 20-minute block than others (because responses were self-paced):

```{r trial counts}
df_study2 %>%
  group_by(lag) %>%
  summarise_at(vars(trials), funs(mean, min, max, sd)) %>%
  kable(caption = "Descriptive statistics for trial counts per lag", digits = 0)
```


## Study 1

These data were used for statistical analysis in London & Slagter (2015)[^ref_london], and were processed by the lead author:

[^ref_london]: London, R. E., & Slagter, H. A. (2015). Effects of transcranial direct current stimulation over left dorsolateral pFC on the attentional blink depend on individual baseline performance. _Journal of Cognitive Neuroscience, 27(12)_, 2382-2393. doi: [10.1162/jocn_a_00867](https://doi.org/10.1162/jocn_a_00867)

```{r read study 1 data}
dataPath_study1 <- here("data","AB-tDCS_study1.txt")
data_study1_fromDisk <- read.table(dataPath_study1, header = TRUE, dec = ",")
glimpse(data_study1_fromDisk)
```

We'll use only a subset of columns, with the header structure `block/stim`\_`target`\_`lag`\_`prime`, where:

* __block/stim__ is either:
    1. `vb`: "anodal" tDCS, "pre" block (before tDCS)
    2. `tb`: "anodal" tDCS, "tDCS" block (during tDCS)
    3. `nb`: "anodal" tDCS, "post" block (after tDCS)
    4. `vd`: "cathodal" tDCS, "pre" block (before tDCS)
    5. `td`: "cathodal" tDCS, "tDCS" block (during tDCS)
    6. `nd`: "cathodal" tDCS, "post" block (after tDCS)
* __target__ is either:
    1.`T1` (T1 accuracy): proportion of trials in which T1 was identified correctly
    2. `T2|T1` (T2|T1 accuracy): proportion of trials in which T2 was identified correctly, given T1 was identified correctly
* __lag__ is either:
    1. `2` (lag 2), when T2 followed T1 after 1 distractor 
    2. `4` (lag 4), when T2 followed T1 after 3 distractors
    3. `10`, (lag 10), when T2 followed T1 after 9 distractors
* __prime__ is either:
    1. `P` (prime): when the stimulus at lag 2 (in lag 4 or lag 10 trials) had the same identity as T2 
    2. `NP` (no prime) when this was not the case. Study 2 had no primes, so we'll only keep these.
    
We'll also keep two more columns: `fileno` (participant ID) and `First_Session` (`1` meaning participants received anodal tDCS in the first session, `2` meaning participants received cathodal tDCS in the first session).

### Reformat

Now we'll reformat the data to match the data frame for study 2:

```{r Function to format data from study 1 as in study 2}
```

```{r format as for study 2}
df_study1 <- format_study2(data_study1_fromDisk)
n_study1 <- n_distinct(df_study1$subject) # number of subjects in study 1
```

```{r print slice study 1 data}
kable(head(df_study1,19), digits = 1, caption = "Data frame for study 1")
```

# Group analysis 

## Line plots cf. London & Slagter (2015) {.tabset .tabset-fade .tabset-pills}

```{r plot lines function}
plot_lines <- function(df) {
  df %>%
  gather(target, proportion_correct, T1, T2.given.T1) %>% # so T1 vs. T2|T1 can be used as a factor
  
  ggplot(aes(lag, proportion_correct, color = block, linetype = target)) +
  facet_wrap(~stimulation) +
  stat_summary(fun.y = mean, geom = "line", aes(group = interaction(target,block)), position = position_dodge(width = 0.2)) +
  stat_summary(fun.data = mean_se, geom = "errorbar", position = position_dodge(width = 0.2)) +
  scale_x_discrete("Lag") +
  scale_y_continuous("Percentage correct", limits = c(0,1), breaks = seq(0,1,.1), labels = scales::percent_format())
}
```

### Study 1

```{r lag line plot study 1, fig.cap="Group effects of tDCS in study 1"}
plot_lines(df_study1)
```

### Study 2

```{r lag line plot study 2, fig.cap="Group effects of tDCS in study 2"}
plot_lines(df_study2)
```

__Stray observations:__

* T1 performance comparable between both studies
* Lag 8 performance is comparable to lag 10 (although a little bit lower)
* Blink at lag 3 is about 10-15 percentage points less than lag 2
* Slopes differ a bit in study 1, in study 2 especially anodal tDCS lag 8 stands out
* In both studies, performance at lag 2/3 across anodal/cathodal tDCS and blocks is basically identical

## RM ANOVA {.tabset .tabset-fade}

### Study 1 - T2|T1

* __DV__: `T2.given.T1`: Proportion of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).
* __Between-subject factor__: `session.order`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS in the first session (`anodal_first` vs. `cathodal_first`).
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 3 levels: Whether T2 followed T1 after 1 distractor (_lag 2_), 3 distractors, (_lag 4_), or after 9 distractors (_lag 10_).
* __Subject identifier__: `subject` (n = `r n_study1`).

```{r rm anova study 1}
aov_study1 <- aov_car(T2.given.T1 ~ session.order + Error(subject/(block*stimulation*lag)), 
        data = df_study1)
kable(nice(aov_study1), caption = "Study 1: RM ANOVA on T2|T1 performance")
```

#### Main effect of lag

This is the attentional blink: T2 is seen more often in the long lag(s).

```{r study 1 - plot lag, fig.cap="Study 1 - Attentional blink (main effect of Lag)"}
afex_plot(aov_study1, x = "lag", error = "within")
```

#### Main effect of block

```{r study 1 - plot block, fig.cap="Study 1 - main effect of Block"}
afex_plot(aov_study1, x = "block", error = "within")
```

Appears to be a time-on-task / fatigue effect: participants get worse each block.

#### Interaction: block by stimulation by lag

The hypothesized effect was that anodal stimulation improves (compared to `pre` block) the attentional blink (short-lag T2 performance), but it was not significant.

```{r study 1 - plot hypothesis, fig.cap="Study 1 - hypothesized interaction: Block by Stimulation by Lag"}
afex_plot(aov_study1, x = "block", trace =  "stimulation", panel = "lag", 
          factor_levels = list(lag = c("lag 2","lag 4","lag 10")), error = "within")
```

Indeed, only a main effect of lag is clearly visible. If anything, the largest difference is in lag 2 though, but in the opposite direction (cathodal slightly improves performance; anodal slightly decreases performance).

#### Interaction: stimulation by session order

```{r study 1 - plot stimulation by session order, fig.cap="Study 1 - Stimulation by Session Order"}
afex_plot(aov_study1, x = "stimulation", trace = "session.order", error = "none")
```

_stimulation_ and _session.order_ are not orthogonal: their interaction makes up a new factor _session_, with 2 levels:

* session 1: 
    1. _session.order_ = anodal first, _stimulation_ = anodal
    2. _session.order_ = cathodal first, _stimulation_ = cathodal
* session 2: 
    1. _session.order_ = anodal first, _stimulation_ = cathodal
    2. _session.order_ = cathodal first, _stimulation_ = anodal

So my take is this "interaction" actually reflects an across-session learning effect: participants simply do better in their 2nd session than in their 1st.

#### Interaction: stimulation by session order by block

There is apparently also a higher order interaction, with _block_:

```{r study 1 - plot threeway, fig.cap="Study 1 - Stimulation by Session Order by Block"}
afex_plot(aov_study1, x = "stimulation", trace = "session.order", panel = "block", error = "none")
```

Seems that the crossover interaction (the learning effect) is mostly present in the first two blocks, not the last. This makes sense intuitively: in the third block of the 1st session, participants already performed the task for 40 minutes, so the difference in "time-on-task" between session 1 and 2 is not so great.

### Study 2 - T2|T1

* __DV__: `T2.given.T1`: Proportion of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 2 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

```{r rm anova study 2}
aov_study2 <- aov_car(T2.given.T1 ~ session.order + Error(subject/(block*stimulation*lag)), 
        data = df_study2)
kable(nice(aov_study2), caption = "Study 2: RM ANOVA on T2|T1 performance")
```

#### Main effect of lag

```{r study 2 - plot lag, fig.cap="Study 2 - Attentional blink (main effect of Lag)"}
afex_plot(aov_study2, x = "lag", error = "within")
```

Again simply the attentional blink.

#### Interaction: block by stimulation by lag

Like in study 1, this was the hypothesized effect, but it is not significant.

```{r study 2 - plot hypothesis, fig.cap="Study 2 - hypothesized interaction: Block by Stimulation by Lag"}
afex_plot(aov_study2, x = "block", trace =  "stimulation", 
          panel = "lag", factor_levels = list(lag = c("lag 3","lag 8")), error = "within")
```

Doesn't look like much, but as it's trending (_p_ = `r round(aov_study2$anova_table["block:stimulation:lag","Pr(>F)"],3)`), let's look at the contrasts anyway:

```{r study 2 - contrast hypothesis}
pairs(emmeans(aov_study2, ~block|stimulation*lag))
```

The only significant change is in the anodal, lag 8 condition: T2 performance goes down compared to baseline. But because this is not the case for the short lag, this should not be considered an effect on the attentional blink.

#### Interaction: stimulation by session order

```{r study 2 - plot stimulation by session order, fig.cap="Study 2 - Stimulation by Session Order"}
afex_plot(aov_study2, x = "stimulation", trace = "session.order", error = "none")
```

In study 1, this interaction also occured and looked like a cross-session learning effect: performance improves in the 2nd session compared to the first. However, here this is only visible for the "anodal first" group.

```{r study 2 - contrast stimulation by session order}
pairs(emmeans(aov_study2, ~stimulation|session.order))
```

Indeed, the difference is only significant for the "anodal first" group. It's unlikely, but in principle this could indicate that anodal has a carryover effect, while cathodal does not.

#### Interaction: stimulation by session order by lag

Apparently this also interacts with Lag (which it also did not do in study 1):

```{r study 2 - plot stimulation by lag by session order, fig.cap="Study 2 - Stimulation by Session Order by Lag"}
afex_plot(aov_study2, x = "stimulation", trace = "session.order", panel = "lag", 
          factor_levels = list(lag = c("lag 3","lag 8")), error = "none")
```

Interesting: the two-way interaction between _stimulation_ and _session order_ shows no learning effect for both groups (only anodal first). However, this higher-order interaction does seem to show a change in the cathodal group also. But the interaction is only visible in the the short lag condition. That makes sense, as there is not much room for improvement in the long lag condition.

#### Interaction: stimulation by session order by lag by block

Then, there is a yet higher order interaction, also with _block_:

```{r study 2 - plot four-way with block, fig.cap="Study 2 - four way interaction, centered on Block"}
afex_plot(aov_study2, x = "stimulation", trace = c("session.order","lag"), 
          panel = "block", data_plot = FALSE, factor_levels = list(lag = c("lag 3","lag 8")), error = "none")
```

This suggests that the three-way interaction (cross-session learning effect for short lag) mostly occurs in the `pre` block. Similar to the three-way interaction in study 1 (_stimulation_ by _session order_ by _block_), the "learning effect across sessions" (crossover in short lag trials) diminishes over blocks. This makes sense because the difference between sessions in "task experience" also diminishes over blocks (in the `pre` block, participants are doing the task for the first time in session 1, but in the later blocks of session 1, they've built up experience).

However, another take on the four-way interaction is the following: it is actually the hypothesized effect (three-way interaction of _stimulation_, _block_, and _lag_), but it only occurs for a certain session order:

```{r study 2 - plot four-way with first session, fig.cap="Study 2 - four way interaction, centered on Session Order"}
afex_plot(aov_study2, x = "block", trace = c("stimulation","lag"), panel = "session.order", data_plot = FALSE,
          factor_levels = list(lag = c("lag 3","lag 8")), error = "none")
```

Most of the changes occur in the "anodal first" group. There we can actually see the hypothesized effect: increased performance on short-lag trials during/after tDCS compared to before. That said, the biggest difference between anodal and cathodal is in the baseline already, and only decreases in the _tDCS_- and _post_-blocks. Also, in the "cathodal first" group, the direction of the "effect" is reversed (and again the differences are largest in the baseline).

Still, lets's look at the hypothesized three-way interaction separately for each "group" (anodal first, cathodal first):

```{r three-way interaction by group}
joint_tests(aov_study2, by = "session.order")
```

Indeed, so the three-way interaction is significant in the "anodal first" group, but not the "cathodal first" group.

Finally, let's also look at the pairwise contrasts for _block_ for each combination of factors:

```{r study 2 - contrast four-way with first session}
pairs(emmeans(aov_study2, ~block|session.order*stimulation*lag))
```

The `pre - post` contrast for lag 3, anodal stimulation in the anodal-first group is indeed significant (but the `pre - tDCS` difference is not). There is also an effect for lag 8, the `pre - tDCS` contrast is significant there (but not `pre - post`). 

---

However, all in all, I favor the 1st interpretation of the four-way interaction (i.e. in terms of the learning effect, which does not occur at lag 8):

* It's more consistent with study 1 (also occurs there, albeit in slightly different form)
* Unless there's really a carryover effect, it's not clear why the hypothesized effect would only occur in the "anodal first" group. A learning effect seems more intuitive.
* The differences between the two stimulation sessions (anodal and cathodal) are most pronounced _before_ tDCS onset.

### Study 2 - T1

* __DV__: `T1`: Proportion of trials in which T1 was identified correctly.
* __Between-subject factor__: `session.order`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS in the first session (`anodal_first` vs. `cathodal_first`).
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 2 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

```{r rm anova study 2 T1}
aov_study2_T1 <- aov_car(T1 ~ session.order + Error(subject/(block*stimulation*lag)), 
        data = df_study2)
kable(nice(aov_study2_T1), caption = "Study 2: RM ANOVA on T1 performance")
```

#### Main effect of lag

```{r study 2 T1- plot lag, fig.cap="Study 2 - T1 main effect of Lag"}
afex_plot(aov_study2_T1, x = "lag", error = "within")
```

Like T2|T1, T1 performance is also worse for the short lag (though only a little). Seems to be mostly driven by a few participants with relatively bad short-lag performance for T1. This effect was also reported in study 1.

#### Main effect of block

```{r study 2 T1 - plot block, fig.cap="Study 2 - T1 main effect of Block"}
afex_plot(aov_study2_T1, x = "block", error = "within")
```

T1 performance seems to decrease slightly over the session. In study 1 this was also the case, for T1 and T2|T1, though here there was no effect of block on T2|T1.

#### Interaction: stimulation by session order

```{r study 2 T1 - plot stimulation by session order, fig.cap="Study 2 - T1: Stimulation by Session Order"}
afex_plot(aov_study2_T1, x = "stimulation", trace = "session.order", error = "none")
```

This interaction was also present for T2|T1 in both studies. There it seemed to reflect a learning effect, but here it goes in the opposite direction: T1 performance is worse in the 2nd session than the first...

#### Interaction: stimulation by session order by block

There is also a higher-order interaction with Block:

```{r study 2 - T1 plot threeway, fig.cap="Study 2 - T1: Stimulation by Session Order by Block"}
afex_plot(aov_study2_T1, x = "stimulation", trace =  "session.order", panel = "block", error = "none")
```

The two-way interaction is strongest in the later two blocks. Again, this is the inverse of the learning effect, which was strongest in the first block.

# Change from baseline

## Calculate change scores

First we calculate attentional blink magnitude: the difference between short-lag and long-lag T2|T1 performance.

```{r Function to calculate AB magnitude}
```

```{r calculate AB magnitude}
ABmag_study1 <- calc_ABmag(df_study1)
ABmag_study2 <- calc_ABmag(df_study2)
kable(head(ABmag_study2,7), digits = 2, caption = "AB magnitude data frame in study 2")
```

* __AB.magnitude__: the difference in T2|T1 performance at the longest lag (study 1: lag 10, study 2: lag 8) vs. the shortest lag (study 1: lag 2, study 2: lag 3)
* __T1.short__: % T1 correct at the short lag, for use as a covariate in the partial correlation analysis

Next, we calculate change from baseline for both of these measures:

```{r Function to calculate change scores}
```

```{r calculate change scores}
ABmagChange_study1 <- calc_change_scores(ABmag_study1)
ABmagChange_study2 <- calc_change_scores(ABmag_study2)
kable(head(ABmagChange_study2,9), digits = 2, caption = "Change scores data frame in study 2")
```

* __baseline__ is the score in the "pre" block for this _measure_ (`AB.magnitude` or `T1.short`)
* __change__ indicates whether the change score is comparing the "pre" block with the "tDCS" block (`tDCS-baseline`) or with the "post" block (`post - baseline`)
* __change.score__ is the difference in the scores between the blocks (as indicated in the _change_ column)

## Plots {.tabset .tabset-fade .tabset-pills}

```{r plot change scores function}
plot_change_from_baseline <- function(df)
  ggplot(filter(df, measure == "AB.magnitude"), aes(baseline, change.score)) +
  facet_grid(change ~ stimulation) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(method = "lm") +
  geom_point() +
  geom_rug() +
  scale_x_continuous("Baseline AB magnitude (%)", breaks = seq(0,1,.2), limits = c(0,1), labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous("Change in AB magnitude (%)", breaks = seq(-.4,.4,.1), limits = c(-.4,.4), labels = scales::percent_format(accuracy = 1))
```

### Study 1

```{r change from baseline plot study 1, fig.cap="Study 1: change from baseline as a function of baseline performance"}
plot_change_from_baseline(ABmagChange_study1)
```

### Study 2 

```{r change from baseline plot study 2, fig.cap="Study 2: change from baseline as a function of baseline performance"}
plot_change_from_baseline(ABmagChange_study2)
```

## Statistics

### Partial correlations  {.tabset .tabset-fade .tabset-pills}

```{r partial correlation change from baseline function}
pcorr_change_baseline <- function(df) {
  df %>%
    ungroup() %>%
    mutate(session.order = as.numeric(session.order)) %>% # dummy code
    nest(baseline, change.score, .key = 'value_col') %>% # combine the two performance columns into a list
    # make 2 separate list-columns: AB.magnitude and T1. short
    spread(key = measure, value = value_col) %>% # each contains two vectors: baseline performance and change score
    unnest(AB.magnitude, T1.short, .sep = '_') %>% # make all 2x2 combinations into 4 columns
    select(-T1.short_baseline) %>% # drop the baseline value for T1.short: not used in partial correlation
    group_by(stimulation,change) %>% # for anodal/cathodal during/after
    # make a data frame out of all 4 columns we need for the partial correlation
    nest(session.order, AB.magnitude_baseline, AB.magnitude_change.score, T1.short_change.score) %>% 
    # partial correlation between baseline and T2|T1 change score, given session order and T1 change score
    mutate(r = map_dbl(data, ~pcor(c("AB.magnitude_baseline","AB.magnitude_change.score",
                                     "session.order", "T1.short_change.score"), var(.)))) %>% 
    group_by(stimulation,change) %>%
    mutate(stats = list(as.data.frame(pcor.test(r, 2, n_distinct(df$subject))))) %>% # significance of partial correlations
    unnest(stats, .drop = TRUE) # combine all into one data frame
}
```

Partial correlation between:

* baseline AB magnitude
* AB magnitude change from baseline (tDCS - baseline; post - baseline)

Given (adjusing for):

* session order
* change from baseline in T1 accuracy at lag 2

#### Study 1

```{r partial correlation change from baseline study 1}
kable(pcorr_change_baseline(ABmagChange_study1),
      digits = 3, caption = "Study 1: partial correlation change from baseline")
```

In study 1, all correlations except for `cathodal, tDCS - baseline` are significant (without correcting for multiple comparisons). The correlation for `anodal, tDCS - baseline` is the strongest.

#### Study 2

```{r partial correlation change from baseline study 2}
kable(pcorr_change_baseline(ABmagChange_study2),
      digits = 3, caption = "Study 2: partial correlation change from baseline")
```

In study 2, only two correlations are significant: both `post - baseline`. So the `anodal, tDCS - baseline` correlation that was the focus of study 1 is not significant here.

### Variance tests {.tabset .tabset-fade .tabset-pills}

There are two problems with assessing the correlation between `baseline` and change from baseline (`retest - baseline`, e.g. `tDCS - baseline`)[^ref_spur_cor].

1. __Mathematical coupling__. The `baseline` term shows up in both variables, introducing a spurious covariance. This may result in a correlation (negative, because `baseline` is subtracted) of up to _r_ = -0.71 (Tu and Gilthorpe, 2007)[^ref_tu], even when `baseline` and `retest` are randomly sampled from the same distribution.
2. __Regression to the mean__. Purely due to measurement error, extreme scores will tend to be less extreme upon another measurement, which also introduces a spurious relation between the two variables.

One solution to regression to the mean is to compare the variances in the baseline and retest. Regression to the mean is a random process, so the variances are expected to be the same. However, if there is truly a relation between `baseline` and `retest - baseline`, the variance in the retest should be less than in the baseline: if high-performers become worse, and low-performers become better, so the scores in the retest should be closer together.

Maloney and Rastogi (1970)[^ref_maloney] (equation in Tu & Gilthorpe (2007)) and Myrtek and Foerster (1986)[^ref_myrtek] (equation in Jin (1992)[^ref_jin]) propose t-statistics for such tests (which are identical). Further (somewhat counterintuitively), Tu & Gilthorpe (2007) show that testing the variance between `baseline` and `retest` is equivalent to testing the significance of the correlation between `baseline - retest` and `baseline + retest` (as in Maloney & Rastogi (1970)). Because the sign is now opposite in both variables, the covariance is no longer biased towards either. This illustrates how variance tests also adress mathematical coupling.

[^ref_spur_cor]: Taken from [this page](http://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/rxxy_correction), which has more extensive introduction to the issue.

[^ref_tu]: Tu, Y. K., & Gilthorpe, M. S. (2007). Revisiting the relation between change and initial value: a review and evaluation. _Statistics in medicine, 26(2)_, 443-457. doi: [10.1002/sim.2538](https://doi.org/10.1002/sim.2538)

[^ref_maloney]: Maloney, C. J., & Rastogi, S. C. (1970). Significance test for Grubbs's estimators. _Biometrics, 26_, 671-676.

[^ref_myrtek]: Myrtek, M., & Foerster, F. (1986). The law of initial value: A rare exception. _Biological Psychology, 22_, 227-239.

[^ref_jin]: Jin, P. (1992). Toward a reconceptualization of the law of initial value. _Psychological Bulletin, 111_, 176-184. doi: [10.1037/0033-2909.111.1.176](http://psycnet.apa.org/doi/10.1037/0033-2909.111.1.176)

```{r variance test function}
var_test <- function(df) {
df %>%
  ungroup() %>%
  select(-T1.short, -session.order) %>% # drop columns we don't need
  spread(block, AB.magnitude) %>% # create 3 columns of scores, one for each block
  gather(condition, retest, tDCS, post) %>% # gather the tdcs and post blocks into one "retest score" column
  unite(comparison, stimulation, condition) %>% # create all 2x2 combinations for the correlations
  group_by(comparison) %>% # for each of these
  nest() %>% # make a data frame out of the test and retest columns
  mutate(stats = map(data, ~as.data.frame(corr_change_baseline(.$pre, .$retest)))) %>% # apply the variance test
  unnest(stats, .drop = TRUE) # unpack list into data frame, drop the data
}
```

Variance tests between:

* baseline AB magnitude (`pre` block)
* retest AB magnitude (`tDCS` or `post` block)


#### Study 1

```{r variance test study 1}
kable(var_test(ABmag_study1), digits = 3, caption = "Study 1: test of variance in baseline vs. retest")
```

Even though three out of four conditions showed a significant negative partial correlation, none of the conditions pass the variance test[^ref_error], suggesting no relation between baseline and change due to tDCS.

[^ref_error]: These values for study 1 are different then in the paper, because of a possible error in the calculation of the variance tests there.

#### Study 2

```{r variance test study 2}
kable(var_test(ABmag_study2), digits = 3, caption = "Study 2: test of variance in baseline vs. retest")
```

In study 2 also all variance tests are not significant, again suggesting the two significant partial correlations are spurious.

# Anodal vs. cathodal

Study 1 reported that the "effects of" anodal and cathodal tDCS (`tDCS - baseline`) are anticorrelated: those participants that improve their performance (smaller AB magnitude) in the anodal session worsen in the cathodal session (and vice versa).

However, this correlation is not clearly present (and is not significant) in study 2:

## Plots {.tabset .tabset-fade .tabset-pills}

```{r Function to plot anodal vs cathodal change scores}
plot_anodalVScathodal <- function(df) {
  df %>% 
    # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
    filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
    select(-baseline) %>%
    spread(stimulation, change.score) %>% 
    
    ggplot(aes(anodal, cathodal)) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_rug() +
    scale_x_continuous("Effect of anodal tDCS (%)", limits = c(-.4,.4), breaks = seq(-.4,.4,.1), labels = scales::percent_format(accuracy = 1)) +
    scale_y_continuous("Effect of cathodal tDCS (%)", limits = c(-.4,.4), breaks = seq(-.4,.4,.1), labels = scales::percent_format(accuracy = 1) ) +
    coord_equal()
}
```

### Study 1

```{r anodal vs. cathodal plot study 1, fig.cap="Effect of anodal vs. effect of cathodal in study 1"}
plot_anodalVScathodal(ABmagChange_study1)
```

### Study 2

```{r anodal vs. cathodal plot study 2, fig.cap="Effect of anodal vs. effect of cathodal in study 2"}
plot_anodalVScathodal(ABmagChange_study2)
```

## Statistics {.tabset .tabset-fade .tabset-pills}

```{r Function to calculate partial correlations of anodal vs cathodal change scores}
```

### Study 1

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order:

```{r partial correlation anodal vs cathodal study 1}
p_r_1 <- pcorr_anodal_cathodal(ABmagChange_study1)
r_study1 <- p_r_1$r
n_study1_pc <- n_study1 - 1 # lose one degree of freedom due to "controlling" for one variable in partial correlation
kable(p_r_1)
```

Zero-order correlation:

```{r zero-order correlation study 1}
r_1 <- tidy(cor.test(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_1)

```

Bayes factor:

```{r Bayes factor study 1}
extractBF(correlationBF(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) %>% # keep only the Bayes Factor
  kable()
```

### Study 2

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), attempting to adjust for session order:

```{r partial correlation anodal vs cathodal study 2}
p_r_2 <- pcorr_anodal_cathodal(ABmagChange_study2)
r_study2 <- p_r_2$r
n_study2_pc <- n_study2 - 1 # lose one degree of freedom due to "controlling" for one variable in partial correlation
kable(p_r_2)
```

Zero-order correlation:

```{r zero-order correlation study 2}
r_2 <- tidy(cor.test(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_2)
```

Bayes factor:

```{r Bayes factor study 2}
bf_study2 <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_study2)
```

## Pool study 1 and 2

As the samples in study 1 (n = `r n_study1`) and 2 (n = `r n_study2`) are on the smaller side, pooling the data from both studies might allow us to infer with more confidence whether the effect exists.

The main difference is that study 1 presented T2 at lag 2, 4 or 10, whereas study 2 used lags 3 and 8. The long lags should be fairly comparable, as they are both well outside the attentional blink window. T2|T1 performance for both is between 80-90 % at the group level (although performance is a little better for lag 10).

However, there is a big difference at the short lags: group-level performance at lag 3 (study 2) is 10-15 percentage points higher than at lag 2. Therefore, when comparing both studies, the best bet is to also create a "lag 3" condition in study 1, by imputing lag 2 and lag 4.

```{r Function to create lag 3 in study 1 data}
```


```{r create lag 3 in study 1}
df_study1_lag3 <- create_lag3_study1(df_study1) 
kable(head(df_study1_lag3,9), digits = 1, caption = "Data frame for study 1 with lag 3 as the short lag")
```

Then we redo the further processing, and combine with the data from study 2:

```{r redo further processing on lag 3}
ABmagChange_pooled <- df_study1_lag3 %>% 
  calc_ABmag() %>% # calculate AB mangnitude
  calc_change_scores() %>% # calculate change from baseline
  bind_rows(.,ABmagChange_study2) %>% # combine with study 2
  mutate(study = ifelse(grepl("^pp",subject), "1", "2")) # add a "study" column, based on subject ID formatting
```

### Plot

Recreate the scatter plot with data from both studies:

```{r anodal vs. cathodal plot pooled, fig.cap="Effect of anodal vs. effect of cathodal pooled across studies"}
plot_anodalVScathodal(ABmagChange_pooled) +
 aes(colour = study)
```

The negative correlation is larger in study 1, and the data points from study 1 have a bigger spread. But overall it doesn't look very convincing still.

### Statistics

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), attempting to adjust for session order:

```{r partial correlation anodal vs cathodal pooled}
pcorr_anodal_cathodal(ABmagChange_pooled) %>%
kable(digits = 3, caption = "Pooled data from studies 1 and 2: Partial correlation anodal vs. cathodal")
```

Zero-order correlation:

```{r zero-order correlation pooled}
tidy(cor.test(
  pull(filter(ABmagChange_pooled, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_pooled, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson")) %>%
kable(., digits = 3, caption = "Pooled data from studies 1 and 2: Zero-order correlation anodal vs. cathodal")
```

Bayes factor:

```{r Bayes factor study pooled}
extractBF(correlationBF(
  pull(filter(ABmagChange_pooled, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_pooled, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) %>% # keep only the Bayes Factor
  kable(digits = 3, caption = "Pooled data from studies 1 and 2: Bayesian correlation")
```

Neither correlation is significant. The Bayes factor favors the null, but only slightly, and less so than in study 2.

## Meta-analysis

For a more orthodox look on the pooled effect from both studies, let's perform a meta-analysis of the correlation coefficients. 

First, create a new data frame with all the information:

```{r data frame for meta-analysis}
df_meta <- tibble(authors = c("London & Slagter","Reteig et al."), year = c(2015,2018), 
                  ni = c(n_study1_pc, n_study2_pc), 
                  ri = c(r_study1, r_study2))
kable(df_meta)
```

* __ni__ is the sample size in each study
* __ri__ is the correlation coefficient (Pearson's _r_)

We'll specify the meta-analysis as a fixed effects model, as both studies were highly similar and from the same population (same lab, same university student sample). This does mean our inferences are limited to this particular set of two studies, but that's also what we want to know in this case ("how large is the effect when we pool both studies"), not necessarily "how large is the true effect in the population" (which would be a random-effects meta-analysis).

```{r meta-analayze}
res <- rma(ri = ri, ni = ni, data = df_meta, 
           measure = "ZCOR", method = "FE", # Fisher-z tranform of r values, fixed effects method
           slab = paste(authors, year, sep = ", ")) # add study info to "res" list
res
```

The overall effect is not significant: _p_ = `r round(res$pval,3)`.

The effect size and confidence intervals printed above are _z_-values, as the meta-analysis was performed on Fisher's _r_-to-_z_ transformed correlation coefficients. Now we transform them back to _r_-values:

```{r transform to r}
kable(predict(res, transf = transf.ztor))
```

* __pred__ is the correlation coefficient _r_
* __ci.lb__ and __ci.ub__ are the upper and lower bounds of the confidence interval around _r_

We can also visualize all of this in a forest plot:

```{r forest plot, fig.cap="Fixed-effects meta-analysis of the anodal vs. cathodal correlation in study 1 and 2"}
forest(res, transf = transf.ztor)
```

This shows the r-values (squares) and CIs (error bars) of the individual studies, as well as the meta-analytic effect size (middle of the diamond) and CI (ends of the diamond). The CIs of the meta-analytic effect just overlap zero, so the overall effect is not significant.

## Prediction intervals

To evaluate whether the result observed in study 2 is consistent with study 1, we can construct a _prediction interval_. A prediction interval contains the range of correlation coefficients we can expect to see in study 2, based on the original correlation in study 1, and the sample sizes of both study 1 and 2. 

If the original study were replicated 100 times, 95 of the observed correlation coefficients would fall within the 95% prediction interval. Note that this is different from a _confidence interval_, which quantifies uncertainty about the (unknown) true correlation in the population (95 out of every hundred 95% CIs contain the true population parameter).

```{r prediction interval}
pi.r(r = r_study1, n = n_study1_pc, rep.n = n_study2_pc)
```

The observed correlation in study 2 (_r_ = `r round(r_study2,2)`) falls outside the 95% PI. Therefore, the results of study 2 are not consistent with study 1, so we could conclude that study 2 was a succesful replication. 

However, the 95% PI is so wide that almost any negative correlation of a realistic size would fall inside it. So above all it illustrates that we cannot draw strong conclusions based on the results of either study.

## Bayes Factors with informed priors

### One-sided

The default Bayes Factor uses a prior that assigns equal weight to positive and negative effect sizes (correlation coefficients in our case). However, we can also "fold" all the prior mass to one side, thereby effectively testing a directional hypothesis.

In our case, based on study 1, we expect a negative correlation, so we evaluate the prior only over negative effect sizes (`-1` to `0`)

```{r one-sided Bayes factor study 2}
bf_one_sided <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  nullInterval = c(-1,0))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_one_sided)
```

This Bayes Factor still provides more evidence for the null than for the alternative, but does provide less evidence for the null (BF~01~ = 1 / `r round(bf_one_sided$bf[1],2)` = `r round(1 / bf_one_sided$bf[1],2)`) than the regular Bayes Factor (BF~01~ = 1 / `r round(bf_study2$bf[1],2)` = `r round(1 / bf_study2$bf[1],2)`)

### "Replication Bayes Factor"

While a default Bayes factor adresses the question ("Is there more evidence that the effect is absent (H~0~) vs. present (H~1~), this Bayes factors adresses the question ("Is there more evidence that the effect is absent (the "skeptic's hypothesis") vs. similar to what was found before ( the "proponent's hypothesis"). The "replication Bayes factor" adresses this latter question by using the posterior of study 1 as a prior in the analysis of study 2, i.e. as the proponent's hypothesis.

This Bayes factor was proposed by Wagenmakers, Verhagen & Ly (2016)[^ref_wagenmakers] and is computed using [their provided code](https://osf.io/9d4ip/). Note that it does not "directly" compute a Bayesian correlation, but uses the effect sizes (partial correlations) from both studies.

```{r replication Bayes Factor}
bf0RStudy1 <- 1/repBfR0(nOri = n_study1_pc, rOri = r_study1,
                        nRep = n_study2_pc, rRep = r_study2)
bf0RStudy1
```

This BF expresses that the data are `r round(bf0RStudy1,2)` times more likely under the skeptic's hypothesis, than under the proponent's hypothesis.

[^ref_wagenmakers]: Wagenmakers, E. J., Verhagen, J., & Ly, A. (2016). How to quantify the evidence for the absence of a correlation. _Behavior Research Methods, 48(2)_, 413-426. doi: [10.3758/s13428-015-0593-0](https://doi.org/10.3758/s13428-015-0593-0)

## Equivalence tests {.tabset .tabset-fade .tabset-pills}

Equivalence tests allow you to test for the _absence_ of an effect of a specific size. Usually this is the smallest effect size of interest (the SESOI). We'll use three different specifications of the SESOI.

### Small telescopes

The "Small Telescopes" approach (Simonsohn, 2015)[^ref_simonsohn] suggests that the SESOI should be the effect size the original study had 33% power to detect, the idea being that anything smaller than that could not have been properly investigated in the original study in the first place (as the odds were already stacked 2:1 against finding the effect).

```{r small telescopes test, fig.cap='test for equivalence to 33% power effect size'}
small_telescopes <- pwr.r.test(n_study1_pc, power = 0.33) # r value with 33% power in study 1
TOSTr(n = n_study2_pc, r = r_study2, 
      high_eqbound_r = small_telescopes$r, 
      low_eqbound_r = -small_telescopes$r, alpha = 0.05) # equivalence test
```

The overall equivalence test is not significant, so we cannot conclude that the effect is smaller than the SESOI. However, given the original study found a negative effect, it makes sense to only look at the lower equivalence bound. The results of this _inferiority_ test is in fact significant, so we can reject the hypothesis that the effect size is at least that negative.

[^ref_simonsohn]: Simonsohn, U. (2015). Small telescopes: Detectability and the evaluation of replication results. _Psychological Science, 26(5),_, 1–11. doi: [10.1177/0956797614567341](https://doi.org/10.1177/0956797614567341)

### Critical effect size

Others (e.g. in the paper accompanying the R package for equivalence test, by Lakens et al. (2018)[^ref_lakens]) argue that a more appropriate SESOI would be the smallest effect size that would still be significant in the original study. This usually corresponds to the effect size the study had 50% power to detect.

First we derive the critical r-value for the original sample size (function below from <https://www.researchgate.net/post/What_is_the_formula_to_calculate_the_critical_value_of_correlation>).

```{r critical r function}
critical.r <- function(n, alpha = .05 ) {
    df <- n - 2
    critical.t <- qt( alpha/2, df, lower.tail = F )
    critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
    return( critical.r )
}
critical.r(n_study1_pc)
```

```{r critical r test, fig.cap='test for equivalence to critical effect size'}
TOSTr(n = n_study2_pc, r = r_study2, 
      high_eqbound_r = critical.r(n_study1_pc), 
      low_eqbound_r = -critical.r(n_study1_pc), 
      alpha = 0.05)
```

When we specify the SESOI like this, the CIs do fall within both equivalence bounds.

[^ref_lakens]: Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. _Advances in Methods and Practices in Psychological Science, 1(2)_, 259–269. doi: [10.1177/2515245918770963](http://doi.org/10.1177/2515245918770963)

### Original effect size

Finally, we can also take the original effect size as the SESOI.

```{r original effect size, fig.cap='test for equivalence to original effect size'}
TOSTr(n = n_study2_pc, r = r_study2, high_eqbound_r =  -r_study1, low_eqbound_r = r_study1, alpha = 0.05)
```

This is the least stringent criterion, so naturally this test is also significant, meaning that based on study 2 we can reject the presence of a correlation at least as large as in study 1.

However, given that the original correlation was not very precisely estimated (the confidence intervals were very wide), this is not really a reasonable SESOI.

# Questionnaires

## Demographics (study 2)

Load and clean (exclude subjects with incomplete data, and S42 (who did not pass the screening, so had no data)):

```{r load subject info}
subject_info <- read_csv2(here("data", "subject_info.csv"), col_names = TRUE, progress = FALSE, col_types = cols(
  first.session = readr::col_factor(c("anodal", "cathodal")),
  gender = readr::col_factor(c("male", "female")),
  age = col_integer())) %>%
  filter(!(subject %in% c(subs_incomplete, "S42"))) %>%
   # recode first.session ("anodal" or "cathodal") to session.order ("anodal first", "cathodal first")
  mutate(first.session = parse_factor(paste(first.session, "first"), 
                                      levels = c("anodal first", "cathodal first"))) %>%
  rename(session.order = first.session)
kable(head(subject_info,5), caption = "Demographics in study 2")
```

Analyze:

```{r gender breakdown}
subject_info %>%
 count(gender) %>%
  kable(caption = "Gender breakdown in study 2")
```

```{r age descriptives}
subject_info %>%
  summarise_at(vars(age), funs(mean, min, max, sd), na.rm = TRUE) %>% # apply summary functions to age column
  kable(digits = 2, caption = "Age descriptives in study 2")
```

```{r session order breakdown}
subject_info %>%
 count(session.order) %>%
  kable(caption = "Session order breakdown in study 2")
```

## tDCS adverse events (study 2)

### Data

```{r load tDCS AEs}
tDCS_AE <- read_csv2(here("data", "tDCS_AE.csv"), col_names = TRUE, progress = FALSE, col_types = cols(
  session = readr::col_factor(c("first", "second")), 
  stimulation = readr::col_factor(c("anodal", "cathodal"))))
glimpse(tDCS_AE)
```

Participants were asked to which degree the following sensations were present during stimulation: _tingling_, _itching sensation_, _burning sensation_, _pain_, _headache_, _fatigue_, _dizziness_ and _nausea_. Each was rated on a scale from 0-4:

0. none
1. a little
2. moderate
3. strong
4. very strong

They also rated their confidence _that the sensations were caused by the stimulation_ on a scale from 0-4 (columns starting with `conf.`):

0. n/a (meaning they rated the sensation a 0 on the previous scale)
1. unlikely
2. possibly
3. likely
4. very likely

__Factors__:

* _subject_: subject ID (`S01`, `S02`, etc)
* _session_: Whether data are from the `first` or `second` session
* _stimulation_: Whether data are from the `anodal` or `cathodal` session

Let's see how many data points we have:

```{r tally sessions}
tDCS_AE %>%
  count(stimulation) %>%
  kable(caption = "number of completed questionnaires per stimulation type")
```

This is more than the number of subjects (and not the same for anodal and cathodal), because there are a few subjects that only did one session.

For further analysis, make the data long form to easily analyse sensations separately:

```{r Make long form data frame}
# Make long form data frame of sensation intensity
intensity <- tDCS_AE %>%
  select(everything(), -contains("conf"), -notes) %>% # drop other columns
  gather(sensation, intensity, itching:nausea) # make long form 

# Make long form data frame of sensation confidence
confidence <- tDCS_AE %>%
  select(contains("conf"), subject, session, stimulation) %>% 
  gather(sensation, confidence, conf.itching:conf.nausea) %>%
  mutate(sensation = str_replace(sensation, "conf.", "")) # get rid of "conf." prefix so it matches the sensation intensity table
```

Let's see which sensations are reported most frequently, along with their mean level of confidence:

```{r AE frequency and confidence}
full_join(intensity,confidence) %>%
  group_by(sensation) %>%
  summarise(count = sum(intensity > 0, na.rm = TRUE), # count all occurences (rating "0" means no occurence)
            mean = mean(confidence, na.rm = TRUE)) %>%
  arrange(desc(count)) %>% # most frequent at the top
  kable(digits = 1)
```

Plot all of the data:

```{r plot AE data}
full_join(intensity,confidence)  %>%
  gather(measure, rating, intensity, confidence) %>% # gather confidence/intensity to make one plot for each
  mutate(measure = factor(measure, levels = c("intensity", "confidence"))) %>%
  
   ggplot(aes(x = rating, fill = stimulation)) +
     facet_grid(measure ~ fct_reorder(sensation, rating, .fun = function(x) sum(x > 0, na.rm = TRUE), .desc = TRUE)) +
    geom_bar(position = "stack") +
    stat_bin(binwidth = 1, geom = "text", size = 2.5, aes(label = ..count..), position = position_stack(vjust = 0.5)) +
    scale_fill_manual(values = c("#F25F5C", "#4B93B1")) +
    xlim(0.5,4.5) + # exclude "0" ratings that were not present
    ylim(0,sum(!is.na(tDCS_AE$stimulation))) + # bound plot at max number of ratings
    ylab("number of sessions") +
    labs(title = "tDCS adverse events", subtitle = "sensations arranged in descending order of frequency")
```

The physical and local sensations (tingling, itching, burning) are most frequent, but these are fairly common and innocuous. Fatigue is also frequent, but there the confidence ratings are highly skewed: participants don't seem to attribute it to the tDCS. This makes sense as they've just done a task for an hour. Headache is still fairly frequent, and has confidence ratings in the middle, which makes sense as headaches often feel "diffuse". Pain, dizziness and nausea are very rare, and generally low in intensity.

## Statistics

Paired Wilcoxon tests for each sensation:

```{r test AEs}
intensity %>%
  filter(!(subject %in% c(subs_incomplete, "S42"))) %>% # set of subjects for which task data are analyzed
  group_by(sensation) %>%
  nest() %>%
  mutate(stats = map(data, ~tidy(wilcox.test(intensity ~ stimulation, paired = TRUE, data = .)))) %>%
  unnest(stats, .drop = TRUE) %>%
  kable(digits = 3, caption = "Paired tests of anodal vs. cathodal for each sensation intensity")
```

