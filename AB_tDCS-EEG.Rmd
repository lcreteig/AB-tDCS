---
title: AB_tDCS-EEG
author: Leon Reteig
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmdformats::html_clean:
    highlight: pygments
    gallery: true
---

# Setup environment

```{r setup}

# Load packages
library(tidyverse)  # importing, transforming, and visualizing data frames
library(here) # (relative) file paths
library(knitr) # R notebook output
library(broom) # transform model outputs into data frames
library(afex) # analysis of factorial experiments (repeated measures anova)
library(emmeans); afex_options(emmeans_model = "multivariate") # post-hoc tests / contrasts, based on multivariate model
library(ggm) # partial correlations
library(BayesFactor) # Bayesian statistics
library(metafor) # meta-analysis
library(predictionInterval) # prediction intervals (for correlations)
library(TOSTER) # equivalence tests
library(pwr) # power analysis
# Print version info
print(sessionInfo())
# Source functions
source(here("src", "func", "behavioral_analysis.R")) # load data from study 2
source(here("src", "lib", "appendixCodeFunctionsJeffreys.R")) # replication Bayes factors
```

To do:

* Repeat analyses on short lag performance
* Get a sense of whether (variance in) short-lag performance drives the correlation, either by modeling it (in the partial correlation), or by coloring dots in the scatter plots with short-lag performance
* Combine the data, by taking the halfway point between lag 2 and lag 4 in study 2 as a proxy for lag 3 performance.
* Repeat analyses with original partial correlation

# Load data

## Study 2

The following participants are excluded from further analysis at this point, because of incomplete data:

* `S03`, `S14`, `S29`, `S38`, `S43`, `S46`: their T1 performance in session 1 was less than 63% correct, so they were not invited back. This cutoff was determined based on a separate pilot study with 10 participants. It is two standard deviations below the mean of that sample.
* `S25` has no data for session 2, as they stopped responding to requests for scheduling the session
* `S31` was excluded as a precaution after session 1, as they developed a severe headache and we could not rule out the possibility this was related to the tDCS

```{r load study 2 data}
dataDir_study2 <- here("data","main") # root folder with AB task data
subs_incomplete <- c("S03", "S14", "S25", "S29", "S31", "S38", "S43", "S46") # don't try to load data from these participants
df_study2 <- load_data_study2(dataDir_study2, subs_incomplete) %>%
  filter(complete.cases(.)) # discard rows with data from incomplete subjects
n_study2 <- n_distinct(df_study2$subject) # number of subjects in study 2
```

```{r print slice study 2 data}
kable(head(df_study2,13), digits = 1, caption = "Data frame for study 2")
```

The data has the following columns:

* __subject__: Participant ID, e.g. `S01`, `S12`
* __first.session__: Whether participant received `anodal` or `cathodal` tDCS in the first session. Codes for session order.
* __stimulation__: Whether participant received `anodal` or `cathodal` tDCS
* __block__: Whether data is before (`pre), during (`tDCS`) or after (`post`) tDCS
* __lag__: Whether T2 followed T1 after two distractors (lag `3`) or after 7 distractors (lag `8`)
* __trials__: Number of trials per lag that the participant completed in this block
* __T1__: Percentage of trials (out of `trials`) in which T1 was identified correctly
* __T2__: Percentage of trials (out of `trials`) in which T2 was identified correctly
* __T2.given.T1__: Percentage of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).

## Study 1

These data were used for statistical analysis in London & Slagter (2015)[^ref_london], and were processed by the lead author:

[^ref_london]: London, R. E., & Slagter, H. A. (2015). Effects of transcranial direct current stimulation over left dorsolateral pFC on the attentional blink depend on individual baseline performance. _Journal of Cognitive Neuroscience, 27(12)_, 2382-2393. doi: [10.1162/jocn_a_00867](https://doi.org/10.1162/jocn_a_00867)

```{r read study 1 data}
dataPath_study1 <- here("data","AB-tDCS_study1.txt")
data_study1_fromDisk <- read.table(dataPath_study1, header = TRUE, dec = ",")
glimpse(data_study1_fromDisk)
```

We'll use only a subset of columns, with the header structure `block/stim`\_`target`\_`lag`\_`prime`, where:

* __block/stim__ is either:
    1. `vb`: "anodal" tDCS, "pre" block (before tDCS)
    2. `tb`: "anodal" tDCS, "tDCS" block (during tDCS)
    3. `nb`: "anodal" tDCS, "post" block (after tDCS)
    4. `vd`: "cathodal" tDCS, "pre" block (before tDCS)
    5. `td`: "cathodal" tDCS, "tDCS" block (during tDCS)
    6. `nd`: "cathodal" tDCS, "post" block (after tDCS)
* __target__ is either:
    1.`T1` (T1 accuracy): proportion of trials in which T1 was identified correctly
    2. `T2|T1` (T2|T1 accuracy): proportion of trials in which T2 was identified correctly, given T1 was identified correctly
* __lag__ is either:
    1. `2` (lag 2), when T2 followed T1 after 1 distractor 
    2. `4` (lag 4), when T2 followed T1 after 3 distractors
    3. `10`, (lag 10), when T2 followed T1 after 9 distractors
* __prime__ is either:
    1. `P` (prime): when the stimulus at lag 2 (in lag 4 or lag 10 trials) had the same identity as T2 
    2. `NP` (no prime) when this was not the case. Study 2 had no primes, so we'll only keep these.
    
We'll also keep two more columns: `fileno` (participant ID) and `First_Session` (`1` meaning participants received anodal tDCS in the first session, `2` meaning participants received cathodal tDCS in the first session).

### Reformat

Now we'll reformat the data to match the data frame for study 2:

```{r format as for study 2}
df_study1 <- data_study1_fromDisk %>%
  select(fileno, First_Session, ends_with("_NP"), -contains("Min")) %>% # keep only relevant columns
  gather(key, accuracy, -fileno, -First_Session) %>% # make long form
  mutate(accuracy = accuracy * 100) %>% # convert from proportion to percentage correct
  # split key column to create separate columns for each factor
  separate(key, c("block", "stimulation", "target", "lag"), c(1,3,5)) %>% # split after 1st, 3rd, and 5th character
  # convert to factors, relabel levels to match those in study 2
  mutate(block = factor(block, levels = c("v", "t", "n"), labels = c("pre", "tDCS", "post")),
         stimulation = factor(stimulation, levels = c("b_", "d_"), labels = c("anodal", "cathodal")),
         lag = factor(lag, levels = c("_2_NP", "_4_NP", "_10_NP"), labels = c(2, 4, 10)),
         First_Session = factor(First_Session, labels = c("anodal","cathodal"))) %>%
  spread(target, accuracy) %>% # create separate columns for T2.given.T1 and T1 performance
  rename(T2.given.T1 = NB, first.session = First_Session, subject = fileno)# rename columns to match those in study 2
n_study1 <- n_distinct(df_study1$subject) # number of subjects in study 1
```

```{r print slice study 1 data}
kable(head(df_study1,19), digits = 1, caption = "Data frame for study 1")
```

# Group analysis 

## Line plots cf. London & Slagter (2015) {.tabset .tabset-fade .tabset-pills}

```{r plot lines function}
plot_lines <- function(df) {
  df %>%
  gather(target, percentage_correct, T1, T2.given.T1) %>% # so T1 vs. T2|T1 can be used as a factor
  
  ggplot(aes(lag, percentage_correct, color = block, linetype = target)) +
  facet_wrap(~stimulation) +
  stat_summary(fun.y = mean, geom = "line", aes(group = interaction(target,block)), position = position_dodge(width = 0.2)) +
  stat_summary(fun.data = mean_se, geom = "errorbar", position = position_dodge(width = 0.2)) +
  scale_x_discrete("Lag") +
  scale_y_continuous("Percentage correct", limits = c(0,100), breaks = seq(0,100,10))
}
```

### Study 1

```{r lag line plot study 1, fig.cap="Group effects of tDCS in study 1"}
plot_lines(df_study1)
```

### Study 2

```{r lag line plot study 2, fig.cap="Group effects of tDCS in study 2"}
plot_lines(df_study2)
```

__Stray observations:__

* T1 performance comparable between both studies
* Lag 8 performance is comparable to lag 10 (although a little bit lower)
* Blink at lag 3 is about 10-15 percentage points less than lag 2
* Slopes differ a bit in study 1, in study 2 especially anodal tDCS lag 8 stands out
* In both studies, performance at lag 2/3 across anodal/cathodal tDCS and blocks is basically identical

## RM ANOVA {.tabset .tabset-fade}

### Study 1 - T2|T1

* __DV__: `T2.given.T1`: Percentage of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).
* __Between-subject factor__: `first.session`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS in the first session. Codes for session order.
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 3 levels: Whether T2 followed T1 after 1 distractor (_lag 2_), 3 distractors, (_lag 4_), or after 9 distractors (_lag 10_).
* __Subject identifier__: `subject` (n = `r n_study1`).

```{r rm anova study 1}
aov_study1 <- aov_car(T2.given.T1 ~ first.session + Error(subject/(block*stimulation*lag)), 
        data = df_study1)
kable(nice(aov_study1), caption = "Study 1: RM ANOVA on T2|T1 performance")
```

#### Main effect of lag

This is the attentional blink: T2 is seen more often in the long lag(s).

```{r study 1 - plot lag, fig.cap="Study 1 - Attentional blink (main effect of Lag)"}
afex_plot(aov_study1, x = "lag")
```

#### Main effect of block

```{r study 1 - plot block, fig.cap="Study 1 - main effect of Block"}
afex_plot(aov_study1, x = "block")
```

Appears to be a time-on-task / fatigue effect: participants get worse each block.

#### Interaction: block by stimulation by lag

The hypothesized effect was that anodal stimulation improves (compared to `pre` block) the attentional blink (short-lag T2 performance), but it was not significant.

```{r study 1 - plot hypothesis, fig.cap="Study 1 - hypothesized interaction: Block by Stimulation by Lag"}
afex_plot(aov_study1, x = "block", trace =  "stimulation", panel = "lag", 
          factor_levels = list(lag = c("lag 2","lag 4","lag 10")))
```

Indeed, only a main effect of lag is clearly visible. If anything, the largest difference is in lag 2 though, but in the opposite direction (cathodal slightly improves performance; anodal slightly decreases performance).

#### Interaction: stimulation by session order

```{r study 1 - plot stimulation by session order, fig.cap="Study 1 - Stimulation by Session Order"}
afex_plot(aov_study1, x = "stimulation", trace = "first.session")
```

_stimulation_ and _first.session_ are not orthogonal: their interaction makes up a new factor _session_, with 2 levels:

* session 1: 
    1. _firstsession_ = anodal, _stimulation_ = anodal
    2. _first.session_ = cathodal, _stimulation_ = cathodal
* session 2: 
    1. _first.session_ = anodal, _stimulation_ = cathodal
    2. _first.session_ = cathodal, _stimulation_ = anodal

So my take is this "interaction" actually reflects an across-session learning effect: participants simply do better in their 2nd session than in their 1st.

#### Interaction: stimulation by session order by block

There is apparently also a higher order interaction, with _block_:

```{r study 1 - plot threeway, fig.cap="Study 1 - Stimulation by Session Order by Block"}
afex_plot(aov_study1, x = "stimulation", trace =  "first.session", panel = "block", 
          factor_levels = list(first.session = c("anodal first","cathodal first")))
```

Seems that the crossover interaction (the learning effect) is mostly present in the first two blocks, not the last. This makes sense intuitively: in the third block of the 1st session, participants already performed the task for 40 minutes, so the difference in "time-on-task" between session 1 and 2 is not so great.

### Study 2 - T2|T1

* __DV__: `T2.given.T1`: Percentage of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 3 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

```{r rm anova study 2}
aov_study2 <- aov_car(T2.given.T1 ~ first.session + Error(subject/(block*stimulation*lag)), 
        data = df_study2)
kable(nice(aov_study2), caption = "Study 2: RM ANOVA on T2|T1 performance")
```

#### Main effect of lag

```{r study 2 - plot lag, fig.cap="Study 2 - Attentional blink (main effect of Lag)"}
afex_plot(aov_study2, x = "lag")
```

Again simply the attentional blink.

#### Interaction: block by stimulation by lag

Like in study 1, this was the hypothesized effect, but it is not significant.

```{r study 2 - plot hypothesis, fig.cap="Study 2 - hypothesized interaction: Block by Stimulation by Lag"}
afex_plot(aov_study2, x = "block", trace =  "stimulation", 
          panel = "lag", factor_levels = list(lag = c("lag 3","lag 8")))
```

Doesn't look like much, but as it's trending (_p_ = `r round(aov_study2$anova_table["block:stimulation:lag","Pr(>F)"],3)`), let's look at the contrasts anyway:

```{r study 2 - contrast hypothesis}
pairs(emmeans(aov_study2, ~block|stimulation*lag))
```

The only significant change is in the anodal, lag 8 condition: T2 performance goes down compared to baseline. But because this is not the case for the short lag, this should not be considered an effect on the attentional blink.

#### Interaction: stimulation by session order

```{r study 2 - plot stimulation by session order, fig.cap="Study 2 - Stimulation by Session Order"}
afex_plot(aov_study2, x = "stimulation", trace = "first.session")
```

In study 1, this interaction also occured and looked like a cross-session learning effect: performance improves in the 2nd session compared to the first. However, here this is only visible for the "anodal first" group.

```{r study 2 - contrast stimulation by session order}
pairs(emmeans(aov_study2, ~stimulation|first.session))
```

Indeed, the difference is only significant for the "anodal first" group. It's unlikely, but in principle this could indicate that anodal has a carryover effect, while cathodal does not.

#### Interaction: stimulation by session order by lag

Apparently this also interacts with Lag (which it also did not do in study 1):

```{r study 2 - plot stimulation by lag by session order, fig.cap="Study 2 - Stimulation by Session Order by Lag"}
afex_plot(aov_study2, x = "stimulation", trace = "first.session", panel = "lag", 
          factor_levels = list(lag = c("lag 3","lag 8")))
```

Interesting: the two-way interaction between _stimulation_ and _session order_ shows no learning effect for both groups (only anodal first). However, this higher-order interaction does seem to show a change in the cathodal group also. But the interaction is only visible in the the short lag condition. That makes sense, as there is not much room for improvement in the long lag condition.

#### Interaction: stimulation by session order by lag by block

Then, there is a yet higher order interaction, also with _block_:

```{r study 2 - plot four-way with block, fig.cap="Study 2 - four way interaction, centered on Block"}
afex_plot(aov_study2, x = "stimulation", trace = c("first.session","lag"), 
          panel = "block", data_plot = FALSE, factor_levels = list(lag = c("lag 3","lag 8")))
```

This suggests that the three-way interaction (cross-session learning effect for short lag) mostly occurs in the `pre` block. Similar to the three-way interaction in study 1 (_stimulation_ by _session order_ by _block_), the "learning effect across sessions" (crossover in short lag trials) diminishes over blocks. This makes sense because the difference between sessions in "task experience" also diminishes over blocks (in the `pre` block, participants are doing the task for the first time in session 1, but in the later blocks of session 1, they've built up experience).

However, another take on the four-way interaction is the following: it is actually the hypothesized effect (three-way interaction of _stimulation_, _block_, and _lag_), but it only occurs for a certain session order:

```{r study 2 - plot four-way with first session, fig.cap="Study 2 - four way interaction, centered on Session Order"}
afex_plot(aov_study2, x = "block", trace = c("stimulation","lag"), panel = "first.session", data_plot = FALSE,
          factor_levels = list(lag = c("lag 3","lag 8"), first.session = c("anodal first","cathodal first")))
```

Most of the changes occur in the "anodal first" group. There we can actually see the hypothesized effect: increased performance on short-lag trials during/after tDCS compared to before.

Lets's therefore look at the hypothesized three-way interaction separately for each "group" (anodal first, cathodal first):

```{r three-way interaction by group}
joint_tests(aov_study2, by = "first.session")
```

Indeed, so the three-way interaction is significant in the "anodal first" group, but not the "cathodal first" group.

Finally, let's also look at the pairwise contrasts for _block_ for each combination of factors:

```{r study 2 - contrast four-way with first session}
pairs(emmeans(aov_study2, ~block|first.session*stimulation*lag))
```

The `pre - post` contrast for lag 3, anodal stimulation in the anodal-first group is indeed significant (but the `pre - tDCS` difference is not). There is also an effect for lag 8, the `pre - tDCS` contrast is significant there (but not `pre - post`). 

---

However, all in all, I favor the 1st interpretation of the four-way interaction (i.e. in terms of the learning effect):

* It's more consistent with study 1 (also occurs there, albeit in slightly different form)
* Unless there's really a carryover effect, it's not clear why the hypothesized effect would only occur in the "anodal first" group. A learning effect seems more intuitive.

### Study 2 - T1

* __DV__: `T1`: Percentage of trials in which T1 was identified correctly.
* __Between-subject factor__: `first.session`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS in the first session. Codes for session order.
* __Within-subject factors__: 
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS 
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 3 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

```{r rm anova study 2 T1}
aov_study2_T1 <- aov_car(T1 ~ first.session + Error(subject/(block*stimulation*lag)), 
        data = df_study2)
kable(nice(aov_study2_T1), caption = "Study 2: RM ANOVA on T1 performance")
```

#### Main effect of lag

```{r study 2 T1- plot lag, fig.cap="Study 2 - T1 main effect of Lag"}
afex_plot(aov_study2_T1, x = "lag")
```

Like T2|T1, T1 performance is also worse for the short lag (though only a little). This was also reported in study 1.

#### Main effect of block

```{r study 2 T1 - plot block, fig.cap="Study 2 - T1 main effect of Block"}
afex_plot(aov_study2_T1, x = "block")
```

T1 performance seems to decrease slightly over the session. In study 1 this was also the case, for T1 and T2|T1, though here there was no effect of block on T2|T1.

#### Interaction: stimulation by session order

```{r study 2 T1 - plot stimulation by session order, fig.cap="Study 2 - T1: Stimulation by Session Order"}
afex_plot(aov_study2_T1, x = "stimulation", trace = "first.session")
```

This interaction was also present for T2|T1 in both studies. There it seemed to reflect a learning effect, but here it goes in the opposite direction: T1 performance is worse in the 2nd session than the first...

#### Interaction: stimulation by session order by block

There is also a higher-order interaction with Block:

```{r study 2 - T1 plot threeway, fig.cap="Study 2 - T1: Stimulation by Session Order by Block"}
afex_plot(aov_study2_T1, x = "stimulation", trace =  "first.session", panel = "block", factor_levels = list(first.session = c("anodal first","cathodal first")))
```

The two-way interaction is strongest in the later two blocks. Again, this is the inverse of the learning effect, which was strongest in the first block.


# Anodal vs. cathodal

## Calculate change scores

First we calculate attentional blink magnitude:

```{r AB magnitude function}
calc_ABmag <- function(df) {
  maxLag = max(as.numeric(levels(df$lag))) # lag 10 in study 1, lag 8 in study 2
  minLag = min(as.numeric(levels(df$lag))) # lag 2 in study 1, lag 3 in study 2
  ABmag <- df %>% 
    group_by(subject, first.session, stimulation, block) %>% # for each unique factor combination
    summarise(AB.magnitude = T2.given.T1[lag == maxLag] - T2.given.T1[lag == minLag], # subtract lags to replace data with AB magnitude,
              T1.short = T1[lag == minLag]) # also keep T1 performance for short lag, to use as a covariate later
}
```

```{r calculate AB magnitude}
ABmag_study1 <- calc_ABmag(df_study1)
ABmag_study2 <- calc_ABmag(df_study2)
kable(head(ABmag_study2,7), digits = 1, caption = "AB magnitude data frame in study 2")
```

* __AB.magnitude__: the difference in T2|T1 performance at the longest lag (study 1: lag 10, study 2: lag 8) vs. the shortest lag (study 1: lag 2, study 2: lag 3)
* __T1.short__: % T1 correct at the short lag, for use as a covariate in the partial correlation analysis

Next, we calculate change from baseline for both of these measures:

```{r change scores function}
calc_change_scores <- function(df) {
  df %>%
  gather(measure, performance, AB.magnitude, T1.short) %>%
  group_by(subject, first.session, stimulation, measure) %>%
  summarise(baseline = performance[block == "pre"], 
  during = performance[block == "tDCS"] - baseline,
  after = performance[block == "post"] - baseline) %>%
  gather(change, change.score, during, after) %>%
  mutate(change = fct_recode(change, "tDCS - baseline" = "during", "post - baseline" = "after"),
         change = fct_relevel(change, "tDCS - baseline")) %>%
  arrange(subject, stimulation)
}
```

```{r calculate change scores}
ABmagChange_study1 <- calc_change_scores(ABmag_study1)
ABmagChange_study2 <- calc_change_scores(ABmag_study2)
kable(head(ABmagChange_study2,9), digits = 1, caption = "Change scores data frame in study 2")
```

* __baseline__ is the score in the "pre" block for this _measure_ (`AB.magnitude` or `T1.short`)
* __change__ indicates whether the change score is comparing the "pre" block with the "tDCS" block (`tDCS-baseline`) or with the "post" block (`post - baseline`)
* __change.score__ is the difference in the scores between the blocks (as indicated in the _change_ column)

## Plots {.tabset .tabset-fade .tabset-pills}

```{r anodal vs. cathodal plot function}
plot_anodalVScathodal <- function(df) {
  df %>% 
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>% 
  
  ggplot(aes(anodal, cathodal)) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_rug() +
    scale_x_continuous("Effect of anodal tDCS (%)", limits = c(-40,40), breaks = seq(-40,40,10) ) +
    scale_y_continuous("Effect of cathodal tDCS (%)", limits = c(-40,40), breaks = seq(-40,40,10) ) +
    coord_equal()
}
```

### Study 1

```{r anodal vs. cathodal plot study 1, fig.cap="Effect of anodal vs. effect of cathodal in study 1"}
plot_anodalVScathodal(ABmagChange_study1)
```

### Study 2

```{r anodal vs. cathodal plot study 2, fig.cap="Effect of anodal vs. effect of cathodal in study 2"}
plot_anodalVScathodal(ABmagChange_study2)
```

## Statistics {.tabset .tabset-fade .tabset-pills}

```{r partial correlation function}
compute_pcorr <- function(df) {
  df %>%
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>%
  ungroup() %>% # remove grouping from previous steps, as we need to modify the dataframe
  select(first.session, anodal, cathodal) %>% # keep only relevant columns
  mutate(first.session = as.numeric(first.session)) %>% # dummy code to use as covariate
  
  # partial correlation  
  summarise(r = pcor(c("anodal","cathodal","first.session"), var(.))) %>% # partial correlation coefficient
  mutate(stats = list(as.data.frame(pcor.test(r, 1, n_distinct(df$subject))))) %>% # t-stat, df and p-value of coefficient
  unnest(stats) # unpack resulting data frame into separate columns %>%
}
```


### Study 1

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order:

```{r partial correlation study 1}
p_r_1 <- compute_pcorr(ABmagChange_study1)
r_study1 <- p_r_1$r
kable(p_r_1)
```

Zero-order correlation:

```{r zero-order correlation study 1}
r_1 <- tidy(cor.test(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_1)

```

Bayes factor:

```{r Bayes factor study 1}
extractBF(correlationBF(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) %>% # keep only the Bayes Factor
  kable(.)
```

### Study 2

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order:

```{r partial correlation study 2}
p_r_2 <- compute_pcorr(ABmagChange_study2)
r_study2 <- p_r_2$r
kable(p_r_2)
```

Zero-order correlation:

```{r zero-order correlation study 2}
r_2 <- tidy(cor.test(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_2)
```

Bayes factor:

```{r Bayes factor study 2}
bf_study2 <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_study2)
```

## Meta-analysis

To investigated the pooled effect from both studies, let's perform a meta-analysis of the correlation coefficients. 

First, create a new data frame with all the information:

```{r data frame for meta-analysis}
df_meta <- tibble(authors = c("London & Slagter","Reteig et al."), year = c(2015,2018), 
                  ni = c(n_study1, n_study2), 
                  ri = c(r_study1, r_study2))
kable(df_meta)
```

* __ni__ is the sample size in each study
* __ri__ is the correlation coefficient (Pearson's _r_)

We'll specify the meta-analysis as a fixed effects model, as both studies were highly similar and from the same population (same lab, same university student sample). This does mean our inferences are limited to this particular set of two studies, but that's also what we want to know in this case ("how large is the effect when we pool both studies"), not necessarily "how large is the true effect in the population" (which would be a random-effects meta-analysis).

```{r meta-analayze}
res <- rma(ri = ri, ni = ni, data = df_meta, 
           measure = "ZCOR", method = "FE", # Fisher-z tranform of r values, fixed effects method
           slab = paste(authors, year, sep = ", ")) # add study info to "res" list
res
```

The overall effect is not significant: _p_ = `r round(res$pval,3)`.

The effect size and confidence intervals printed above are _z_-values, as the meta-analysis was performed on Fisher's _r_-to-_z_ transformed correlationn coefficients. Now we transform them back to _r_-values:

```{r transform to r}
kable(predict(res, transf = transf.ztor))
```

* __pred__ is the correlation coefficient _r_
* __ci.lb__ and __ci.ub__ are the upper and lower bounds of the confidence interval around _r_

We can also visualize all of this in a forest plot:

```{r forest plot, fig.cap="Fixed-effects meta-analysis of the anodal vs. cathodal correlation in study 1 and 2"}
forest(res, transf = transf.ztor)
```

This shows the r-values (squares) and CIs (error bars) of the individual studies, as well as the meta-analytic effect size (middle of the diamond) and CI (ends of the diamond). The CIs of the meta-analytic effect just overlap zero, so the overall effect is not significant.

## Prediction intervals

To evaluate whether the result observed in study 2 is consistent with study 1, we can construct a _prediction interval_. A prediction interval contains the range of correlation coefficients we can expect to see in study 2, based on the original correlation in study 1, and the sample sizes of both study 1 and 2. 

If the original study were replicated 100 times, 95 of the observed correlation coefficients would fall within the 95% prediction interval. Note that this is different from a _confidence interval_, which quantifies uncertainty about the (unknown) true correlation in the population (95 out of every hundred 95% CIs contain the true population parameter).

```{r prediction interval}
pi.r(r = r_study1, n = n_study1, rep.n = n_study2)
```

The observed correlation in study 2 (_r_ = `r round(r_study2,2)`) falls within the 95% PI. Therefore, the results of study 2 are technically consistent with study 1, so we could conclude that study 2 was a succesful replication. 

However, the 95% PI is so wide that almost any negative correlation of a realistic size would fall inside it (it even extends beyond 0 a little). So above all it illustrates that we cannot draw strong conclusions based on the results of either study.

## Bayes Factors with informed priors

### One-sided

The default Bayes Factor uses a prior that assigns equal weight to positive and negative effect sizes (correlation coefficients in our case). However, we can also "fold" all the prior mass to one side, thereby effectively testing a directional hypothesis.

In our case, based on study 1, we expect a negative correlation, so we evaluate the prior only over negative effect sizes (`-1` to `0`)

```{r one-sided Bayes factor study 2}
bf_one_sided <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  nullInterval = c(-1,0))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_one_sided)
```

This Bayes Factor still provides more evidence for the null than for the alternative, but does provide less evidence for the null (BF~01~ = 1 / `r round(bf_one_sided$bf[1],2)` = `r round(1 / bf_one_sided$bf[1],2)`) than the regular Bayes Factor (BF~01~ = 1 / `r round(bf_study2$bf[1],2)` = `r round(1 / bf_study2$bf[1],2)`)

### "Replication Bayes Factor"

While a default Bayes factor adresses the question ("Is there more evidence that the effect is absent (H~0~) vs. presentt (H~1~), this Bayes factors adresses the question ("Is there more evidence that the effect is absent (the "skeptic's hypothesis") vs. similar to what was found before ( the "proponent's hypothesis"). The "replication Bayes factor" adresses this latter question by using the posterior of study 1 as a prior in the analysis of study 2, i.e. as the proponent's hypothesis.

This Bayes factor was proposed by Wagenmakers, Verhagen & Ly (2016)[^ref_wagenmakers] and is computed using [their provided code](https://osf.io/9d4ip/).

```{r replication Bayes Factor}
bf0RStudy1 <- 1/repBfR0(nOri = n_study1, rOri = r_study1,
                        nRep = n_study2, rRep = r_study2)
bf0RStudy1
```

This BF expresses that the data are `r round(bf0RStudy1,2)` times more likely under the skeptic's hypothesis, than under the proponent's hypothesis.

[^ref_wagenmakers]: Wagenmakers, E. J., Verhagen, J., & Ly, A. (2016). How to quantify the evidence for the absence of a correlation. _Behavior Research Methods, 48(2)_, 413-426. doi: [10.3758/s13428-015-0593-0](https://doi.org/10.3758/s13428-015-0593-0)

## Equivalence tests {.tabset .tabset-fade .tabset-pills}

Equivalence tests allow you to test for the _absence_ of an effect of a specific size. Usually this is the smallest effect size of interest (the SESOI). We'll use three different specifications of the SESOI.

### Small telescopes

The "Small Telescopes" approach (Simonsohn, 2015)[^ref_simonsohn] suggests that the SESOI should be the effect size the original study had 33% power to detect, the idea being that anything smaller than that could not have been properly investigated in the original study in the first place (as the odds were already stacked 2:1 against finding the effect).

```{r small telescopes test, fig.cap='test for equivalence to 33% power effect size'}
small_telescopes <- pwr.r.test(n_study1, power = 0.33) # r value with 33% power in study 1
TOSTr(n = n_study2, r = r_study2, 
      high_eqbound_r = small_telescopes$r, 
      low_eqbound_r = -small_telescopes$r, alpha = 0.05) # equivalence test
```

[^ref_simonsohn]: Simonsohn, U. (2015). Small telescopes: Detectability and the evaluation of replication results. _Psychological Science, _, 1–11. doi: [10.1177/0956797614567341](https://doi.org/10.1177/0956797614567341)

### Critical effect size

Others (e.g. in the paper accompanying the R package for equivalence tesst, by Lakens et al. (2018)[^ref_lakens]) argue that a more appropriate SESOI would be the smallest effect size that would still be significant in the original study. This usually corresponds to the effect size the study had 50% power to detect.

First we derive the critical r-value for the original sample size (function below from <https://www.researchgate.net/post/What_is_the_formula_to_calculate_the_critical_value_of_correlation>).

```{r critical r function}
critical.r <- function(n, alpha = .05 ) {
    df <- n - 2
    critical.t <- qt( alpha/2, df, lower.tail = F )
    critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
    return( critical.r )
}
```

```{r critical r test, fig.cap='test for equivalence to critical effect size'}
TOSTr(n = n_study2, r = r_study2, 
      high_eqbound_r = critical.r(n_study1), 
      low_eqbound_r = -critical.r(n_study1), 
      alpha = 0.05)
```

[^ref_lakens]: Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. _Advances in Methods and Practices in Psychological Science, 1(2)_, 259–269. doi: [10.1177/2515245918770963](http://doi.org/10.1177/2515245918770963)

### Original effect size

Finally, we can also take the original effect size as the SESOI.

```{r original effect size, fig.cap='test for equivalence to original effect size'}
TOSTr(n = n_study2, r = r_study2, high_eqbound_r =  -r_study1, low_eqbound_r = r_study1, alpha = 0.05)
```

Here the effect size is actually significant, so based on study 2 we can reject the presence of a correlation at least as large as in study 1.

However, given that the original correlation was not very precisely estimated (the confidence intervals were very wide), this is not really a reasonable SESOI.