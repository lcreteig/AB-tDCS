---
title: AB_tDCS-EEG
author: Leon Reteig
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmdformats::html_clean:
    highlight: pygments
    gallery: true
---

# Setup environment

```{r setup}

# Load packages
library(tidyverse)  # importing, transforming, and visualizing data frames
library(here) # (relative) file paths
library(knitr) # R notebook output
library(broom) # transform model outputs into data frames
library(ggm) # partial correlations
library(BayesFactor) # Bayesian statistics
library(metafor) # meta-analysis
library(predictionInterval) # prediction intervals (for correlations)
# Print version info
print(sessionInfo())
# Source functions
source(here("src", "func", "behavioral_analysis.R")) # load data from study 2
source(here("src", "lib", "appendixCodeFunctionsJeffreys.R")) # replication Bayes factors
```

# Load data

## Study 2

The following participants are excluded from further analysis at this point, because of incomplete data:

* `S03`, `S14`, `S29`, `S38`, `S43`, `S46`: their T1 performance in session 1 was less than 63% correct, so they were not invited back. This cutoff was determined based on a separate pilot study with 10 participants. It is two standard deviations below the mean of that sample.
* `S25` has no data for session 2, as they stopped responding to requests for scheduling the session
* `S31` was excluded as a precaution after session 1, as they developed a severe headache and we could not rule out the possibility this was related to the tDCS

```{r load study 2 data}
dataDir_study2 <- here("data","main") # root folder with AB task data
subs_incomplete <- c("S03", "S14", "S25", "S29", "S31", "S38", "S43", "S46") # don't try to load data from these participants
df_study2 <- load_data_study2(dataDir_study2, subs_incomplete) %>%
  filter(complete.cases(.)) # discard rows with data from incomplete subjects
```

```{r print slice study 2 data}
kable(head(df_study2,13), digits = 1, caption = "Data frame for study 2")
```

The data has the following columns:

* __subject__: Participant ID, e.g. `S01`, `S12
* __first.session__: Whether the first session participant received `anodal` or `cathodal` tDCS in the first session. Codes for session order.
* __stimulation__: Whether participant received `anodal` or `cathodal` tDCS
* __block__: Whether data is before (`pre), during (`tDCS`) or after (`post`) tDCS
* __lag__: Whether T2 followed T1 after two distractors (lag `3`) or after 7 distractors (lag `8`)
* __trials__: Number of trials per lag that the participant completed in this block
* __T1__: Percentage of trials (out of `trials`) in which T1 was identified correctly
* __T2__: Percentage of trials (out of `trials`) in which T2 was identified correctly
* __T2.given.T1__: Percentage of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).

## Study 1

These data were used for statistical analysis in London & Slagter (2015)[^ref_london], and were processed by the lead author:

[^ref_london]: London, R. E., & Slagter, H. A. (2015). Effects of transcranial direct current stimulation over left dorsolateral pFC on the attentional blink depend on individual baseline performance. _Journal of Cognitive Neuroscience, 27(12)_, 2382-2393. doi: [10.1162/jocn_a_00867](https://doi.org/10.1162/jocn_a_00867)

```{r read study 1 data}
dataPath_study1 <- here("data","AB-tDCS_study1.txt")
data_study1_fromDisk <- read.table(dataPath_study1, header = TRUE, dec = ",")
glimpse(data_study1_fromDisk)
```

We'll use only a subset of columns, with the header structure `block/stim`\_`target`\_`lag`\_`prime`, where:

* __block/stim__ is either:
    1. `vb`: "anodal" tDCS, "pre" block (before tDCS)
    2. `tb`: "anodal" tDCS, "tDCS" block (during tDCS)
    3. `nb`: "anodal" tDCS, "post" block (after tDCS)
    4. `vd`: "cathodal" tDCS, "pre" block (before tDCS)
    5. `td`: "cathodal" tDCS, "tDCS" block (during tDCS)
    6. `nd`: "cathodal" tDCS, "post" block (after tDCS)
* __target__ is either:
    1.`T1` (T1 accuracy): proportion of trials in which T1 was identified correctly
    2. `T2|T1` (T2|T1 accuracy): proportion of trials in which T2 was identified correctly, given T1 was identified correctly
* __lag__ is either:
    1. `2` (lag 2), when T2 followed T1 after 1 distractor 
    2. `4` (lag 4), when T2 followed T1 after 3 distractors
    3. `10`, (lag 10), when T2 followed T1 after 9 distractors
* __prime__ is either:
    1. `P` (prime): when the stimulus at lag 2 (in lag 4 or lag 10 trials) had the same identity as T2 
    2. `NP` (no prime) when this was not the case. Study 2 had no primes, so we'll only keep these.
    
We'll also keep two more columns: `fileno` (participant ID) and `First_Session` (`1` meaning participants received anodal tDCS in the first session, `2` meaning participants received cathodal tDCS in the first session).

### Reformat

Now we'll reformat the data to match the data frame for study 2:

```{r format as for study 2}
df_study1 <- data_study1_fromDisk %>%
  select(fileno, First_Session, ends_with("_NP"), -contains("Min")) %>% # keep only relevant columns
  gather(key, accuracy, -fileno, -First_Session) %>% # make long form
  mutate(accuracy = accuracy * 100) %>% # convert from proportion to percentage correct
  # split key column to create separate columns for each factor
  separate(key, c("block", "stimulation", "target", "lag"), c(1,3,5)) %>% # split after 1st, 3rd, and 5th character
  # convert to factors, relabel levels to match those in study 2
  mutate(block = factor(block, levels = c("v", "t", "n"), labels = c("pre", "tDCS", "post")),
         stimulation = factor(stimulation, levels = c("b_", "d_"), labels = c("anodal", "cathodal")),
         lag = factor(lag, levels = c("_2_NP", "_4_NP", "_10_NP"), labels = c(2, 4, 10)),
         First_Session = factor(First_Session, labels = c("anodal","cathodal"))) %>%
  spread(target, accuracy) %>% # create separate columns for T2.given.T1 and T1 performance
  rename(T2.given.T1 = NB, first.session = First_Session, subject = fileno)# rename columns to match those in study 2
```

```{r print slice study 1 data}
kable(head(df_study1,19), digits = 1, caption = "Data frame for study 1")
```

# Group analysis 

## Line plots cf. London & Slagter (2015) {.tabset .tabset-fade .tabset-pills}

```{r plot lines function}
plot_lines <- function(df) {
  df %>%
  gather(target, percentage_correct, T1, T2.given.T1) %>% # so T1 vs. T2|T1 can be used as a factor
  
  ggplot(aes(lag, percentage_correct, color = block, linetype = target)) +
  facet_wrap(~stimulation) +
  stat_summary(fun.y = mean, geom = "line", aes(group = interaction(target,block)), position = position_dodge(width = 0.2)) +
  stat_summary(fun.data = mean_se, geom = "errorbar", position = position_dodge(width = 0.2)) +
  scale_x_discrete("Lag") +
  scale_y_continuous("Percentage correct", limits = c(0,100), breaks = seq(0,100,10))
}
```

### Study 1

```{r lag line plot study 1, fig.cap="Group effects of tDCS in study 1"}
plot_lines(df_study1)
```

### Study 2

```{r lag line plot study 2, fig.cap="Group effects of tDCS in study 2"}
plot_lines(df_study2)
```

__Stray observations:__

* T1 performance comparable between both studies
* Lag 8 performance is comparable to lag 10 (although a little bit lower)
* Blink at lag 3 is about 10-15 percentage points less than lag 2
* Slopes differ a bit in study 1, in study 2 especially anodal tDCS lag 8 stands out
* In both studies, performance at lag 2/3 across anodal/cathodal tDCS and blocks is basically identical

# Anodal vs. cathodal

## Calculate change scores

First we calculate attentional blink magnitude:

```{r AB magnitude function}
calc_ABmag <- function(df) {
  maxLag = max(as.numeric(levels(df$lag))) # lag 10 in study 1, lag 8 in study 2
  minLag = min(as.numeric(levels(df$lag))) # lag 2 in study 1, lag 3 in study 2
  ABmag <- df %>% 
    group_by(subject, first.session, stimulation, block) %>% # for each unique factor combination
    summarise(AB.magnitude = T2.given.T1[lag == maxLag] - T2.given.T1[lag == minLag], # subtract lags to replace data with AB magnitude,
              T1.short = T1[lag == minLag]) # also keep T1 performance for short lag, to use as a covariate later
}
```

```{r calculate AB magnitude}
ABmag_study1 <- calc_ABmag(df_study1)
ABmag_study2 <- calc_ABmag(df_study2)
kable(head(ABmag_study2,7), digits = 1, caption = "AB magnitude data frame in study 2")
```

* __AB.magnitude__: the difference in T2|T1 performance at the longest lag (study 1: lag 10, study 2: lag 8) vs. the shortest lag (study 1: lag 2, study 2: lag 3)
* __T1.short__: % T1 correct at the short lag, for use as a covariate in the partial correlation analysis

Next, we calculate change from baseline for both of these measures:

```{r change scores function}
calc_change_scores <- function(df) {
  df %>%
  gather(measure, performance, AB.magnitude, T1.short) %>%
  group_by(subject, first.session, stimulation, measure) %>%
  summarise(baseline = performance[block == "pre"], 
  during = performance[block == "tDCS"] - baseline,
  after = performance[block == "post"] - baseline) %>%
  gather(change, change.score, during, after) %>%
  mutate(change = fct_recode(change, "tDCS - baseline" = "during", "post - baseline" = "after"),
         change = fct_relevel(change, "tDCS - baseline")) %>%
  arrange(subject, stimulation)
}
```

```{r calculate change scores}
ABmagChange_study1 <- calc_change_scores(ABmag_study1)
ABmagChange_study2 <- calc_change_scores(ABmag_study2)
kable(head(ABmagChange_study2,9), digits = 1, caption = "Change scores data frame in study 2")
```

* __baseline__ is the score in the "pre" block for this _measure_ (`AB.magnitude` or `T1.short`)
* __change__ indicates whether the change score is comparing the "pre" block with the "tDCS" block (`tDCS-baseline`) or with the "post" block (`post - baseline`)
* __change.score__ is the difference in the scores between the blocks (as indicated in the _change_ column)

## Plots {.tabset .tabset-fade .tabset-pills}

```{r anodal vs. cathodal plot function}
plot_anodalVScathodal <- function(df) {
  df %>% 
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>% 
  
  ggplot(aes(anodal, cathodal)) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_vline(xintercept = 0, linetype = "dashed") +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_rug() +
    scale_x_continuous("Effect of anodal tDCS (%)", limits = c(-40,40), breaks = seq(-40,40,10) ) +
    scale_y_continuous("Effect of cathodal tDCS (%)", limits = c(-40,40), breaks = seq(-40,40,10) ) +
    coord_equal()
}
```

### Study 1

```{r anodal vs. cathodal plot study 1, fig.cap="Effect of anodal vs. effect of cathodal in study 1"}
plot_anodalVScathodal(ABmagChange_study1)
```

### Study 2

```{r anodal vs. cathodal plot study 2, fig.cap="Effect of anodal vs. effect of cathodal in study 2"}
plot_anodalVScathodal(ABmagChange_study2)
```

## Statistics {.tabset .tabset-fade .tabset-pills}

```{r partial correlation function}
compute_pcorr <- function(df) {
  df %>%
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>%
  ungroup() %>% # remove grouping from previous steps, as we need to modify the dataframe
  select(first.session, anodal, cathodal) %>% # keep only relevant columns
  mutate(first.session = as.numeric(first.session)) %>% # dummy code to use as covariate
  
  # partial correlation  
  summarise(r = pcor(c("anodal","cathodal","first.session"), var(.))) %>% # partial correlation coefficient
  mutate(stats = list(as.data.frame(pcor.test(r, 1, 34)))) %>% # t-stat, df and p-value of coefficient
  unnest(stats) # unpack resulting data frame into separate columns %>%
}
```


### Study 1

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order:

```{r partial correlation study 1}
kable(compute_pcorr(ABmagChange_study1))
```

Zero-order correlation:

```{r zero-order correlation study 1}
r_1 <- tidy(cor.test(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_1)
```

Bayes factor:

```{r Bayes factor study 1}
extractBF(correlationBF(
  pull(filter(ABmagChange_study1, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study1, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) %>% # keep only the Bayes Factor
  kable(.)
```

### Study 2

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order:

```{r partial correlation study 2}
kable(compute_pcorr(ABmagChange_study2))
```

Zero-order correlation:

```{r zero-order correlation study 2}
r_2 <- tidy(cor.test(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  method = "pearson"))
kable(r_2)
```

Bayes factor:

```{r Bayes factor study 2}
bf_study2 <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_study2)
```

## Meta-analysis

To investigated the pooled effect from both studies, let's perform a meta-analysis of the correlation coefficients. 

First, create a new data frame with all the information:

```{r data frame for meta-analysis}
df_meta <- tibble(authors = c("London & Slagter","Reteig et al."), year = c(2015,2018), 
                  ni = c(n_distinct(df_study1$subject), n_distinct(df_study2$subject)), 
                  ri = c(r_1$estimate, r_2$estimate))
kable(df_meta)
```

* __ni__ is the sample size in each study
* __ri__ is the correlation coefficient (Pearson's _r_)

We'll specify the meta-analysis as a fixed effects model, as both studies were highly similar and from the same population (same lab, same university student sample). This does mean our inferences are limited to this particular set of two studies, but that's also what we want to know in this case ("how large is the effect when we pool both studies"), not necessarily "how large is the true effect in the population" (which would be a random-effects meta-analysis).

```{r meta-analayze}
res <- rma(ri = ri, ni = ni, data = df_meta, 
           measure = "ZCOR", method = "FE", # Fisher-z tranform of r values, fixed effects method
           slab = paste(authors, year, sep = ", ")) # add study info to "res" list
res
```

The overall effect is not significant: _p_ = `r round(res$pval,3)`.

The effect size and confidence intervals printed above are _z_-values, as the meta-analysis was performed on Fisher's _r_-to-_z_ transformed correlationn coefficients. Now we transform them back to _r_-values:

```{r transform to r}
kable(predict(res, transf = transf.ztor))
```

* __pred__ is the correlation coefficient _r_
* __ci.lb__ and __ci.ub__ are the upper and lower bounds of the confidence interval around _r_

We can also visualize all of this in a forest plot:

```{r forest plot, fig.cap="Fixed-effects meta-analysis of the anodal vs. cathodal correlation in study 1 and 2"}
forest(res, transf = transf.ztor)
```

This shows the r-values (squares) and CIs (error bars) of the individual studies, as well as the meta-analytic effect size (middle of the diamond) and CI (ends of the diamond). The CIs of the meta-analytic effect just overlap zero, so the overall effect is not significant.

## Prediction intervals

To evaluate whether the result observed in study 2 is consistent with study 1, we can construct a _prediction interval_. A prediction interval contains the range of correlation coefficients we can expect to see in study 2, based on the original correlation in study 1, and the sample sizes of both study 1 and 2. 

If the original study were replicated 100 times, 95 of the observed correlation coefficients would fall within the 95% prediction interval. Note that this is different from a _confidence interval_, which quantifies uncertainty about the (unknown) true correlation in the population (95 out of every hundred 95% CIs contain the true population parameter).

```{r prediction interval}
pi.r(r = r_1$estimate, n = n_distinct(df_study1$subject), rep.n = n_distinct(df_study2$subject))
```

The observed correlation in study 2 (_r_ = `r round(r_2$estimate,2)`) falls within the 95% PI. Therefore, the results of study 2 are technically consistent with study 1, so we could conclude that study 2 was a succesful replication. 

However, the 95% PI is so wide that almost any negative correlation of a realistic size would fall inside it (it even extends beyond 0 a little). So above all it illustrates that we cannot draw strong conclusions based on the results of either study.

## Bayes Factors with informed priors

### One-sided

The default Bayes Factor uses a prior that assigns equal weight to positive and negative effect sizes (correlation coefficients in our case). However, we can also "fold" all the prior mass to one side, thereby effectively testing a directional hypothesis.

In our case, based on study 1, we expect a negative correlation, so we evaluate the prior only over negative effect sizes (`-1` to `0`)

```{r one-sided Bayes factor study 2}
bf_one_sided <- extractBF(correlationBF(
  pull(filter(ABmagChange_study2, stimulation == "anodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  pull(filter(ABmagChange_study2, stimulation == "cathodal", measure == "AB.magnitude", change == "tDCS - baseline"), change.score),
  nullInterval = c(-1,0))) %>%
  select(bf) # keep only the Bayes Factor

  kable(bf_one_sided)
```

This Bayes Factor still provides more evidence for the null than for the alternative, but does provide less evidence for the null (BF~01~ = 1 / `r round(bf_one_sided$bf[1],2)` = `r round(1 / bf_one_sided$bf[1],2)`) than the regular Bayes Factor (BF~01~ = 1 / `r round(bf_study2$bf[1],2)` = `r round(1 / bf_study2$bf[1],2)`)

### "Replication Bayes Factor"

While a default Bayes factor adresses the question ("Is there more evidence that the effect is absent (H~0~) vs. presentt (H~1~), this Bayes factors adresses the question ("Is there more evidence that the effect is absent (the "skeptic's hypothesis") vs. similar to what was found before ( the "proponent's hypothesis"). The "replication Bayes factor" adresses this latter question by using the posterior of study 1 as a prior in the analysis of study 2, i.e. as the proponent's hypothesis.

This Bayes factor was proposed by Wagenmakers, Verhagen & Ly (2016)[^ref_wagenmakers] and is computed using [their provided code](https://osf.io/9d4ip/).

```{r replication Bayes Factor}
bf0RStudy1 <- 1/repBfR0(nOri = n_distinct(df_study1$subject), rOri = r_1$estimate,
                        nRep = n_distinct(df_study2$subject), rRep = r_2$estimate)
bf0RStudy1
```

This BF expresses that the data are `r round(bf0RStudy1,2)` times more likely under the skeptic's hypothesis, than under the proponent's hypothesis.

[^ref_wagenmakers]: Wagenmakers, E. J., Verhagen, J., & Ly, A. (2016). How to quantify the evidence for the absence of a correlation. _Behavior Research Methods, 48(2)_, 413-426. doi: [10.3758/s13428-015-0593-0](https://doi.org/10.3758/s13428-015-0593-0)