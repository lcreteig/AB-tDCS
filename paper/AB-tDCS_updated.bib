
@article{Amrhein2019,
	title = {Inferential {Statistics} as {Descriptive} {Statistics}: {There} {Is} {No} {Replication} {Crisis} if {We} {Don}’t {Expect} {Replication}},
	volume = {73},
	issn = {0003-1305},
	shorttitle = {Inferential {Statistics} as {Descriptive} {Statistics}},
	url = {https://doi.org/10.1080/00031305.2018.1543137},
	doi = {10.1080/00031305.2018.1543137},
	abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
	number = {sup1},
	urldate = {2021-01-26},
	journal = {The American Statistician},
	author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1543137},
	keywords = {Auxiliary hypotheses, Confidence interval, Hypothesis test, P-value, Posterior probability, Replication, Selective reporting, Significance test, Statistical model, Unreplicable research},
	pages = {262--270},
	file = {amrhein_2019_inferential_statistics_as_descriptive_statistics_-_there_is_no_replication.pdf:/Users/leonreteig/surfdrive/zotero-library/Amrhein/amrhein_2019_inferential_statistics_as_descriptive_statistics_-_there_is_no_replication.pdf:application/pdf}
}

@article{piperExactReplicationFoundation2019,
	title = {Exact replication: {Foundation} of science or game of chance?},
	volume = {17},
	issn = {1545-7885},
	shorttitle = {Exact replication},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000188},
	doi = {10.1371/journal.pbio.3000188},
	abstract = {The need for replication of initial results has been rediscovered only recently in many fields of research. In preclinical biomedical research, it is common practice to conduct exact replications with the same sample sizes as those used in the initial experiments. Such replication attempts, however, have lower probability of replication than is generally appreciated. Indeed, in the common scenario of an effect just reaching statistical significance, the statistical power of the replication experiment assuming the same effect size is approximately 50\%—in essence, a coin toss. Accordingly, we use the provocative analogy of “replicating” a neuroprotective drug animal study with a coin flip to highlight the need for larger sample sizes in replication experiments. Additionally, we provide detailed background for the probability of obtaining a significant p value in a replication experiment and discuss the variability of p values as well as pitfalls of simple binary significance testing in both initial preclinical experiments and replication studies with small sample sizes. We conclude that power analysis for determining the sample size for a replication study is obligatory within the currently dominant hypothesis testing framework. Moreover, publications should include effect size point estimates and corresponding measures of precision, e.g., confidence intervals, to allow readers to assess the magnitude and direction of reported effects and to potentially combine the results of initial and replication study later through Bayesian or meta-analytic approaches.},
	language = {en},
	number = {4},
	urldate = {2021-01-26},
	journal = {PLOS Biology},
	author = {Piper, Sophie K. and Grittner, Ulrike and Rex, Andre and Riedel, Nico and Fischer, Felix and Nadon, Robert and Siegerink, Bob and Dirnagl, Ulrich},
	month = apr,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Animal studies, Drug therapy, Head, Metaanalysis, Neuroprotectives, Replication studies, Scientists},
	pages = {e3000188},
	file = {piper_2019_exact_replication_-_foundation_of_science_or_game_of_chance.pdf:/Users/leonreteig/surfdrive/zotero-library/Piper/piper_2019_exact_replication_-_foundation_of_science_or_game_of_chance.pdf:application/pdf}
}

@article{Hedges2019,
	title = {More {Than} {One} {Replication} {Study} {Is} {Needed} for {Unambiguous} {Tests} of {Replication}},
	volume = {44},
	issn = {1076-9986},
	url = {https://doi.org/10.3102/1076998619852953},
	doi = {10.3102/1076998619852953},
	abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98\%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
	language = {en},
	number = {5},
	urldate = {2021-01-26},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	month = oct,
	year = {2019},
	note = {Publisher: American Educational Research Association},
	keywords = {educational policy, evaluation, experimental design, meta-analysis, program evaluation, research methodology, validity/reliability},
	pages = {543--570},
	file = {hedges_schauer_2019_more_than_one_replication_study_is_needed_for_unambiguous_tests_of_replication.pdf:/Users/leonreteig/surfdrive/zotero-library/Hedges_Schauer/hedges_schauer_2019_more_than_one_replication_study_is_needed_for_unambiguous_tests_of_replication.pdf:application/pdf}
}

@misc{EvaluatingEffectSize,
	title = {Evaluating {Effect} {Size} in {Psychological} {Research}: {Sense} and {Nonsense} - {David} {C}. {Funder}, {Daniel} {J}. {Ozer}, 2019},
	url = {https://journals.sagepub.com/doi/10.1177/2515245919847202},
	urldate = {2021-01-26},
	file = {evaluating_effect_size_in_psychological_research_-_sense_and_nonsense_-_david_c..pdf:/Users/leonreteig/surfdrive/zotero-library/undefined/evaluating_effect_size_in_psychological_research_-_sense_and_nonsense_-_david_c..pdf:application/pdf}
}

@article{harmsBayesFactorReplications2019,
	title = {A {Bayes} {Factor} for {Replications} of {ANOVA} {Results}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1518787},
	doi = {10.1080/00031305.2018.1518787},
	abstract = {With an increasing number of replication studies performed in psychological science, the question of how to evaluate the outcome of a replication attempt deserves careful consideration. Bayesian approaches allow to incorporate uncertainty and prior information into the analysis of the replication attempt by their design. The Replication Bayes factor, introduced by Verhagen and Wagenmakers (2014), provides quantitative, relative evidence in favor or against a successful replication. In previous work by Verhagen and Wagenmakers (2014), it was limited to the case of t-tests. In this article, the Replication Bayes factor is extended to F-tests in multigroup, fixed-effect ANOVA designs. Simulations and examples are presented to facilitate the understanding and to demonstrate the usefulness of this approach. Finally, the Replication Bayes factor is compared to other Bayesian and frequentist approaches and discussed in the context of replication attempts. R code to calculate Replication Bayes factors and to reproduce the examples in the article is available at https://osf.io/jv39h/.},
	number = {4},
	urldate = {2021-01-26},
	journal = {The American Statistician},
	author = {Harms, Christopher},
	month = oct,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1518787},
	keywords = {ANOVA designs, Bayes factor, Replications},
	pages = {327--339},
	file = {harms_2019_a_bayes_factor_for_replications_of_anova_results.pdf:/Users/leonreteig/surfdrive/zotero-library/Harms/harms_2019_a_bayes_factor_for_replications_of_anova_results.pdf:application/pdf}
}

@techreport{Muradchanian2020,
	title = {How {Best} to {Quantify} {Replication} {Success}? {A} {Simulation} {Study} on the {Comparison} of {Replication} {Success} {Metrics}},
	shorttitle = {How {Best} to {Quantify} {Replication} {Success}?},
	url = {https://osf.io/preprints/metaarxiv/wvdjf/},
	abstract = {To overcome the frequently debated crisis of confidence, replicating studies is becoming increasingly more common. Multiple frequentist and Bayesian measures have been proposed to evaluate whether a replication is successful, but little is known about which method best captures replication success. We studied this in a simulation study, by comparing a number of quantitative measures of replication success with respect to their ability to draw the correct inference when the underlying truth is known, while taking publication bias into account. Our results show that Bayesian metrics seem to slightly outperform frequentist metrics across the board. Generally, meta-analytic approaches seem to slightly outperform metrics that evaluate single studies, except in the scenario of extreme publication bias, where this pattern reverses.},
	urldate = {2021-01-26},
	institution = {MetaArXiv},
	author = {Muradchanian, Jasmine and Hoekstra, Rink and Kiers, Henk and Ravenzwaaij, Don van},
	month = aug,
	year = {2020},
	doi = {10.31222/osf.io/wvdjf},
	note = {type: article},
	keywords = {Economics, Other Social and Behavioral Sciences, Psychology, publication bias, replication success, Social and Behavioral Sciences},
	file = {muradchanian_2020_how_best_to_quantify_replication_success_-_a_simulation_study_on_the_comparison.pdf:/Users/leonreteig/surfdrive/zotero-library/Muradchanian/muradchanian_2020_how_best_to_quantify_replication_success_-_a_simulation_study_on_the_comparison.pdf:application/pdf}
}

@incollection{Zondervan-Zwijnenburg2020,
	address = {Abingdon, Oxon ; New York, NY},
	title = {Testing {Replication} with {Small} {Samples}},
	isbn = {978-0-429-27387-2},
	language = {en},
	booktitle = {Small sample size solutions: a guide for applied researchers and practitioners},
	publisher = {Routledge},
	author = {Zondervan-Zwijnenburg, Mariëlle and Rijshouwer, Dominique},
	editor = {van de Schoot, Rens and Miočević, Milica},
	year = {2020},
	keywords = {Data sets, Methodology, Research},
	file = {zondervan-zwijnenburg_rijshouwer_2020_testing_replication_with_small_samples.pdf:/Users/leonreteig/Zotero/storage/NQTJWTIK/zondervan-zwijnenburg_rijshouwer_2020_testing_replication_with_small_samples.pdf:application/pdf}
}

@article{LeBel2019,
	title = {A {Brief} {Guide} to {Evaluate} {Replications}},
	volume = {3},
	copyright = {Copyright (c) 2019 Etienne Philippe LeBel, Wolf Vanpaemel, Irene Cheung, Lorne Campbell},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/843},
	doi = {10.15626/MP.2018.843},
	language = {en},
	urldate = {2021-01-26},
	journal = {Meta-Psychology},
	author = {LeBel, Etienne Philippe and Vanpaemel, Wolf and Cheung, Irene and Campbell, Lorne},
	month = jun,
	year = {2019},
	keywords = {transparency, reproducibility, direct replication, replicability, evaluating replications},
	file = {lebel_2019_a_brief_guide_to_evaluate_replications.pdf:/Users/leonreteig/surfdrive/zotero-library/LeBel/lebel_2019_a_brief_guide_to_evaluate_replications.pdf:application/pdf}
}

@article{heldNewStandardAnalysis2020,
	title = {A new standard for the analysis and design of replication studies},
	volume = {183},
	copyright = {© 2019 The Authors Journal of the Royal Statistical Society: Series A (Statistics in Society) Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-985X},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12493},
	doi = {https://doi.org/10.1111/rssa.12493},
	abstract = {A new standard is proposed for the evidential assessment of replication studies. The approach combines a specific reverse Bayes technique with prior-predictive tail probabilities to define replication success. The method gives rise to a quantitative measure for replication success, called the sceptical p-value. The sceptical p-value integrates traditional significance of both the original and the replication study with a comparison of the respective effect sizes. It incorporates the uncertainty of both the original and the replication effect estimates and reduces to the ordinary p-value of the replication study if the uncertainty of the original effect estimate is ignored. The framework proposed can also be used to determine the power or the required replication sample size to achieve replication success. Numerical calculations highlight the difficulty of achieving replication success if the evidence from the original study is only suggestive. An application to data from the Open Science Collaboration project on the replicability of psychological science illustrates the methodology proposed.},
	language = {en},
	number = {2},
	urldate = {2021-01-26},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Held, Leonhard},
	year = {2020},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12493},
	keywords = {Power, Prior–data conflict, Replication success, Reverse Bayes technique, Sample size, Sceptical p-value},
	pages = {431--448},
	file = {held_2020_a_new_standard_for_the_analysis_and_design_of_replication_studies.pdf:/Users/leonreteig/surfdrive/zotero-library/Held/held_2020_a_new_standard_for_the_analysis_and_design_of_replication_studies.pdf:application/pdf}
}

@article{Linde2020,
	title = {Decisions {About} {Equivalence}: {A} {Comparison} of {TOST}, {HDI}-{ROPE}, and the {Bayes} {Factor}},
	shorttitle = {Decisions {About} {Equivalence}},
	url = {https://osf.io/bh8vu},
	abstract = {Some important research questions require the ability to ﬁnd evidence for two conditions being practically equivalent. This is impossible to accomplish within the traditional frequentist null hypothesis signiﬁcance testing framework; hence, other methodologies must be utilized. We explain and illustrate three approaches for ﬁnding evidence for equivalence: The frequentist two one-sided tests procedure, the Bayesian highest density interval region of practical equivalence procedure, and the Bayes factor interval null procedure. We compare the classiﬁcation performances of these three approaches for various plausible scenarios. The results indicate that the Bayes factor interval null approach compares favorably to the other two approaches in terms of statistical power. Critically, compared to the Bayes factor interval null procedure, the two one-sided tests and the highest density interval region of practical equivalence procedures have limited discrimination capabilities when the sample size is relatively small: speciﬁcally, in order to be practically useful, these two methods generally require over 250 cases within each condition when rather large equivalence margins of approximately 0.2 or 0.3 are used; for smaller equivalence margins even more cases are required. Because of these results, we recommend that researchers rely more on the Bayes factor interval null approach for quantifying evidence for equivalence, especially for studies that are constrained on sample size.},
	language = {en},
	urldate = {2021-01-26},
	journal = {PsyArXiv},
	author = {Linde, Maximilian and Tendeiro, Jorge and Selker, Ravi and Wagenmakers, Eric-Jan and van Ravenzwaaij, Don},
	month = nov,
	year = {2020},
	doi = {10.31234/osf.io/bh8vu},
	file = {linde_2020_decisions_about_equivalence_-_a_comparison_of_tost,_hdi-rope,_and_the_bayes.pdf:/Users/leonreteig/Zotero/storage/CE49K9IK/linde_2020_decisions_about_equivalence_-_a_comparison_of_tost,_hdi-rope,_and_the_bayes.pdf:application/pdf}
}

@article{Lakens2020,
	title = {Improving {Inferences} {About} {Null} {Effects} {With} {Bayes} {Factors} and {Equivalence} {Tests}},
	volume = {75},
	issn = {1079-5014},
	url = {https://doi.org/10.1093/geronb/gby065},
	doi = {10.1093/geronb/gby065},
	abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.},
	number = {1},
	urldate = {2021-01-26},
	journal = {The Journals of Gerontology: Series B},
	author = {Lakens, Daniël and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
	month = jan,
	year = {2020},
	pages = {45--57},
	file = {lakens_2020_improving_inferences_about_null_effects_with_bayes_factors_and_equivalence_tests.pdf:/Users/leonreteig/surfdrive/zotero-library/Lakens/lakens_2020_improving_inferences_about_null_effects_with_bayes_factors_and_equivalence_tests.pdf:application/pdf}
}

@techreport{leplaaBayesianEvaluationReplication2020,
	title = {Bayesian evaluation of replication studies},
	url = {https://psyarxiv.com/49tbz/},
	abstract = {In this paper a method is proposed to determine whether the result from an original study is corroborated in a replication study. The paper is illustrated using data from the reproducibility project psychology by the Open Science Collaboration. This method emphasizes the need to determine what one wants to replicate: the hypotheses as formulated in the introduction of the original paper, or hypotheses derived from the research results presented in the original paper. The Bayes factor will be used to determine whether the hypotheses evaluated in/resulting from the original study are corroborated by the replication study. Our method to assess the successfulness of replication will better fit the needs and desires of researchers in fields that use replication studies.},
	urldate = {2021-01-26},
	institution = {PsyArXiv},
	author = {Leplaa, Hidde Jelmer and Rietbergen, Charlotte and Hoijtink, Herbert},
	month = may,
	year = {2020},
	doi = {10.31234/osf.io/49tbz},
	note = {type: article},
	keywords = {Meta-science},
	file = {leplaa_2020_bayesian_evaluation_of_replication_studies.pdf:/Users/leonreteig/surfdrive/zotero-library/Leplaa/leplaa_2020_bayesian_evaluation_of_replication_studies.pdf:application/pdf}
}

@article{schauerReconsideringStatisticalMethods2020,
	title = {Reconsidering {Statistical} {Methods} for {Assessing} {Replication}},
	issn = {1082-989X},
	url = {https://www.scholars.northwestern.edu/en/publications/reconsidering-statistical-methods-for-assessing-replication},
	doi = {10.1037/met0000302},
	language = {English (US)},
	urldate = {2021-01-26},
	journal = {Psychological methods},
	author = {Schauer, J. M. and Hedges, L. V.},
	year = {2020},
	note = {Publisher: American Psychological Association Inc.}
}

@article{London2021,
	title = {No {Effect} of {Transcranial} {Direct} {Current} {Stimulation} over {Left} {Dorsolateral} {Prefrontal} {Cortex} on {Temporal} {Attention}},
	volume = {33},
	issn = {0898-929X},
	url = {https://doi.org/10.1162/jocn_a_01679},
	doi = {10.1162/jocn_a_01679},
	abstract = {Selection mechanisms that dynamically gate only relevant perceptual information for further processing and sustained representation in working memory are critical for goal-directed behavior. We examined whether this gating process can be modulated by transcranial direct current stimulation (tDCS) over left dorsolateral prefrontal cortex (lDLPFC)—a region known to play a key role in working memory and conscious access. Specifically, we examined the effects of tDCS on the magnitude of the “attentional blink” (AB), a deficit in identifying the second of two targets presented in rapid succession. Thirty-four participants performed an AB task before (baseline), during and after 20 min of 1-mA anodal and cathodal tDCS in two separate sessions. On the basis of previous reports linking individual differences in AB magnitude to individual differences in DLPFC activity and on the basis of suggestions that effects of tDCS depend on baseline brain activity levels, we hypothesized that anodal tDCS over lDLPFC would modulate the magnitude of the AB as a function of individual baseline AB magnitude. Behavioral results did not provide support for this hypothesis. At the group level, we also did not observe any significant effects of tDCS, and a Bayesian analysis revealed strong evidence that tDCS to lDLPFC did not affect AB performance. Together, these findings do not support the idea that there is an optimal level of prefrontal cortical excitability for cognitive function. More generally, they add to a growing body of work that challenges the idea that the effects of tDCS can be predicted from baseline levels of behavior.},
	number = {4},
	urldate = {2021-03-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {London, Raquel E. and Slagter, Heleen A.},
	month = apr,
	year = {2021},
	pages = {756--768},
	file = {Full Text PDF:/Users/leonreteig/Zotero/storage/HSVLJK4L/London and Slagter - 2021 - No Effect of Transcranial Direct Current Stimulati.pdf:application/pdf;Snapshot:/Users/leonreteig/Zotero/storage/9DSEPPCR/No-Effect-of-Transcranial-Direct-Current.html:text/html}
}

@article{Sdoia2019,
	title = {Access to consciousness of briefly presented visual events is modulated by transcranial direct current stimulation of left dorsolateral prefrontal cortex},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-47527-4},
	doi = {10.1038/s41598-019-47527-4},
	abstract = {Adaptive behaviour requires the ability to process goal-relevant events at the expense of irrelevant ones. However, perception of a relevant visual event can transiently preclude access to consciousness of subsequent events — a phenomenon called attentional blink (AB). Here we investigated involvement of the left dorsolateral prefrontal cortex (DLPFC) in conscious access, by using transcranial direct current stimulation (tDCS) to potentiate or reduce neural excitability in the context of an AB task. In a sham-controlled experimental design, we applied between groups anodal or cathodal tDCS over the left DLPFC, and examined whether this stimulation modulated the proportion of stimuli that were consciously reported during the AB period. We found that tDCS over the left DLPFC affected the proportion of consciously perceived target stimuli. Moreover, anodal and cathodal tDCS had opposing effects, and exhibited different temporal patterns. Anodal stimulation attenuated the AB, enhancing conscious report earlier in the AB period. Cathodal stimulation accentuated the AB, reducing conscious report later in the AB period. These findings support the notion that the DLPFC plays a role in facilitating information transition from the unconscious to the conscious stage of processing.},
	language = {en},
	number = {1},
	urldate = {2021-05-13},
	journal = {Scientific Reports},
	author = {Sdoia, Stefano and Conversi, David and Pecchinenda, Anna and Ferlazzo, Fabio},
	month = jul,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {10950},
	file = {Sdoia et al_2019_Access to consciousness of briefly presented visual events is modulated by.pdf:/Users/leonreteig/Zotero/storage/VU3WY3R4/Sdoia et al_2019_Access to consciousness of briefly presented visual events is modulated by.pdf:application/pdf;Snapshot:/Users/leonreteig/Zotero/storage/2BS8GZRZ/s41598-019-47527-4.html:text/html}
}

@article{Brandt2014,
	title = {The {Replication} {Recipe}: {What} makes for a convincing replication?},
	volume = {50},
	issn = {0022-1031},
	shorttitle = {The {Replication} {Recipe}},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103113001819},
	doi = {10.1016/j.jesp.2013.10.005},
	abstract = {Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.},
	language = {en},
	urldate = {2021-05-13},
	journal = {Journal of Experimental Social Psychology},
	author = {Brandt, Mark J. and IJzerman, Hans and Dijksterhuis, Ap and Farach, Frank J. and Geller, Jason and Giner-Sorolla, Roger and Grange, James A. and Perugini, Marco and Spies, Jeffrey R. and van 't Veer, Anna},
	month = jan,
	year = {2014},
	keywords = {Pre-registration, Replication, Research method, Solid Science, Statistical power},
	pages = {217--224},
	file = {Brandt et al_2014_The Replication Recipe.pdf:/Users/leonreteig/Zotero/storage/5QP2CPBK/Brandt et al_2014_The Replication Recipe.pdf:application/pdf;ScienceDirect Snapshot:/Users/leonreteig/Zotero/storage/PAYRNTNM/S0022103113001819.html:text/html}
}

@article{Anderson2016,
	title = {There’s more than one way to conduct a replication study: {Beyond} statistical significance.},
	volume = {21},
	issn = {1939-1463, 1082-989X},
	shorttitle = {There’s more than one way to conduct a replication study},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000051},
	doi = {10.1037/met0000051},
	abstract = {As the field of psychology struggles to trust published findings, replication research has begun to become more of a priority to both scientists and journals. With this increasing emphasis placed on reproducibility, it is essential that replication studies be capable of advancing the field. However, we argue that many researchers have been only narrowly interpreting the meaning of replication, with studies being designed with a simple statistically significant or nonsignificant results framework in mind. Although this interpretation may be desirable in some cases, we develop a variety of additional “replication goals” that researchers could consider when planning studies. Even if researchers are aware of these goals, we show that they are rarely used in practice—as results are typically analyzed in a manner only appropriate to a simple significance test. We discuss each goal conceptually, explain appropriate analysis procedures, and provide 1 or more examples to illustrate these analyses in practice. We hope that these various goals will allow researchers to develop a more nuanced understanding of replication that can be flexible enough to answer the various questions that researchers might seek to understand.},
	language = {en},
	number = {1},
	urldate = {2021-05-13},
	journal = {Psychological Methods},
	author = {Anderson, Samantha F. and Maxwell, Scott E.},
	year = {2016},
	pages = {1--12},
	file = {Anderson and Maxwell - 2016 - There’s more than one way to conduct a replication.pdf:/Users/leonreteig/Zotero/storage/BF95RT4U/Anderson and Maxwell - 2016 - There’s more than one way to conduct a replication.pdf:application/pdf}
}

@article{Reteig2017,
	title = {Transcranial {Electrical} {Stimulation} as a {Tool} to {Enhance} {Attention}},
	volume = {1},
	issn = {2509-3304},
	url = {https://doi.org/10.1007/s41465-017-0010-y},
	doi = {10.1007/s41465-017-0010-y},
	abstract = {Attention is a fundamental cognitive process—without it, we would be helplessly adrift in an overload of sensory input. There is considerable interest in techniques that can be used to enhance attention, including transcranial electrical stimulation (tES). We present an overview of 52 studies that have paired attention tasks with tES, mostly in the form of transcranial direct current stimulation (tDCS). In particular, we discuss four aspects of attention that have been most extensively targeted to date: visual search, spatial orienting (e.g., Posner cueing tasks), spatial bias (e.g., line bisection tasks), and sustained attention. Some promising results have been reported in each of these domains. However, drawing general conclusions about the efficacy of tES is at present hampered by a large diversity in study design and inconsistent findings. We highlight some pitfalls and opportunities and suggest how these may be addressed in future research aiming to use tES as a tool to enhance or test theoretical hypotheses about attention.},
	language = {en},
	number = {1},
	urldate = {2021-05-13},
	journal = {Journal of Cognitive Enhancement},
	author = {Reteig, L. C. and Talsma, L. J. and van Schouwenburg, M. R. and Slagter, H. A.},
	month = mar,
	year = {2017},
	pages = {10--25},
	file = {Reteig et al_2017_Transcranial Electrical Stimulation as a Tool to Enhance Attention.pdf:/Users/leonreteig/Zotero/storage/FPWZFJJ8/Reteig et al_2017_Transcranial Electrical Stimulation as a Tool to Enhance Attention.pdf:application/pdf}
}

@article{deBoer2021,
	title = {The effect of non-invasive brain stimulation on executive functioning in healthy controls: {A} systematic review and meta-analysis},
	volume = {125},
	issn = {0149-7634},
	shorttitle = {The effect of non-invasive brain stimulation on executive functioning in healthy controls},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763421000282},
	doi = {10.1016/j.neubiorev.2021.01.013},
	abstract = {In recent years, there has been a heightened interest in the effect of non-invasive brain stimulation on executive functioning. However, there is no comprehensive overview of its effects on different executive functioning domains in healthy individuals. Here, we assessed the state of the field by conducting a systematic review and meta-analysis on the effectiveness of non-invasive brain stimulation (i.e. repetitive transcranial magnetic stimulation and transcranial direct current stimulation) over prefrontal regions on tasks assessing working memory, inhibition, flexibility, planning and initiation performance. Our search yielded 63 studies (n = 1537), and the effectiveness of excitatory and inhibitory non-invasive brain stimulation were assessed per executive functioning task. Our analyses showed that excitatory non-invasive brain stimulation had a small but positive effect on Stop Signal Task and Go/No-Go Task performance, and that inhibitory stimulation had a small negative effect on Flanker Task performance. Non-invasive brain stimulation did not affect performance on working memory and flexibility tasks, and effects on planning tasks were inconclusive.},
	language = {en},
	urldate = {2021-05-13},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {de Boer, Nina S. and Schluter, Renée S. and Daams, Joost G. and van der Werf, Ysbrand D. and Goudriaan, Anna E. and van Holst, Ruth J.},
	month = jun,
	year = {2021},
	keywords = {Executive functioning, Flexibility, Healthy individuals, Inhibition, Initiation, Meta-analysis, Non-invasive brain stimulation, Planning, Transcranial electrical current stimulation, Transcranial magnetic stimulation, Working memory},
	pages = {122--147},
	file = {ScienceDirect Snapshot:/Users/leonreteig/Zotero/storage/XAJK6ZDW/S0149763421000282.html:text/html}
}

@article{Filmer2020,
	title = {Modulating brain activity and behaviour with {tDCS}: {Rumours} of its death have been greatly exaggerated},
	volume = {123},
	issn = {0010-9452},
	shorttitle = {Modulating brain activity and behaviour with {tDCS}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010945219303570},
	doi = {10.1016/j.cortex.2019.10.006},
	abstract = {Transcranial electrical brain stimulation (tES) techniques have shown substantial promise in research and applied settings. However, over the last few years the technique has courted significant controversy, resulting in scepticism regarding its reported beneficial effects and future potential. In this opinion article, we examine the key points of criticism raised to date, including whether tES has any meaningful effect on the cortex, issues of replicability, and the variability in its efficacy across individuals. For each point, we assess the strength of the evidence for and against the argument and, where relevant, suggest how the field can improve. We conclude that while some of the highlighted shortcomings of research using electrical brain stimulation are justified, on balance the arguments against using such techniques in cognitive neuroscience are often overstated and elevate the risk of the field “throwing the baby out with the bath water”.},
	language = {en},
	urldate = {2021-05-13},
	journal = {Cortex},
	author = {Filmer, Hannah L. and Mattingley, Jason B. and Dux, Paul E.},
	month = feb,
	year = {2020},
	keywords = {Brain stimulation, Transcranial direct current stimulation, Transcranial electrical stimulation},
	pages = {141--151},
	file = {ScienceDirect Snapshot:/Users/leonreteig/Zotero/storage/6W8RDTDB/S0010945219303570.html:text/html}
}

@article{Morey2016,
	title = {Why most of psychology is statistically unfalsifiable},
	url = {https://zenodo.org/record/838685},
	doi = {10.5281/zenodo.838685},
	urldate = {2021-05-13},
	author = {Morey, Richard and Lakens, Daniël},
	month = oct,
	year = {2016}
}

@article{Filmer2019b,
	title = {The efficacy of transcranial direct current stimulation to prefrontal areas is related to underlying cortical morphology},
	volume = {196},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2019.04.026},
	abstract = {Applying a weak electrical current to the cortex can have effects on a range of behaviours. Techniques such as transcranial direct current stimulation (tDCS) have been widely used in both research and clinical settings. However, there is significant variability across individuals in terms of their responsiveness to stimulation, which poses practical challenges to the application of tDCS, but also provides a unique opportunity to study the link between the brain and behaviour. Here, we assessed the role of individual differences in cortical morphology - specifically in prefrontal cortical regions of interest - for determining the influence of tDCS on decision-making performance. Specifically, we employed magnetic resonance imaging (MRI) and a previously replicated paradigm in which we modulated learning in a simple decision-making task by applying tDCS to the left prefrontal cortex in human subjects of both sexes. Cortical thickness of the left (but not right) prefrontal cortex accounted for almost 35\% of the variance in stimulation efficacy across subjects. This is the first demonstration that variations in cortical architecture are associated with reliable differences in the effects of tDCS on cognition. Our findings have important implications for predicting the likely efficacy of different non-invasive brain stimulation treatments on a case by case basis.},
	language = {eng},
	journal = {NeuroImage},
	author = {Filmer, Hannah L. and Ehrhardt, Shane E. and Shaw, Thomas B. and Mattingley, Jason B. and Dux, Paul E.},
	month = aug,
	year = {2019},
	pmid = {30978491},
	keywords = {Adult, Brain stimulation, Cortical morphology, Decision Making, Female, Humans, Individual differences, Magnetic Resonance Imaging, Male, Models, Neurological, Prefrontal cortex, Prefrontal Cortex, tDCS, Transcranial Direct Current Stimulation, Young Adult},
	pages = {41--48}
}

@article{Horne2021,
	title = {Evidence against benefits from cognitive training and transcranial direct current stimulation in healthy older adults},
	volume = {5},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-00979-5},
	doi = {10.1038/s41562-020-00979-5},
	abstract = {Cognitive training and brain stimulation show promise for ameliorating age-related neurocognitive decline. However, evidence for this is controversial. In a Registered Report, we investigated the effects of these interventions, where 133 older adults were allocated to four groups (left prefrontal cortex anodal transcranial direct current stimulation (tDCS) with decision-making training, and three control groups) and trained over 5 days. They completed a task/questionnaire battery pre- and post-training, and at 1- and 3-month follow-ups. COMT and BDNF Val/Met polymorphisms were also assessed. Contrary to work in younger adults, there was evidence against tDCS-induced training enhancement on the decision-making task. Moreover, there was evidence against transfer of training gains to untrained tasks or everyday function measures at any post-intervention time points. As indicated by exploratory work, individual differences may have influenced outcomes. But, overall, the current decision-making training and tDCS protocol appears unlikely to lead to benefits for older adults.},
	language = {en},
	number = {1},
	urldate = {2021-05-13},
	journal = {Nature Human Behaviour},
	author = {Horne, Kristina S. and Filmer, Hannah L. and Nott, Zoie E. and Hawi, Ziarih and Pugsley, Kealan and Mattingley, Jason B. and Dux, Paul E.},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {146--158},
	file = {Snapshot:/Users/leonreteig/Zotero/storage/7ZGEGNM3/s41562-020-00979-5.html:text/html}
}
