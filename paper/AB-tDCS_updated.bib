
@article{amrheinInferentialStatisticsDescriptive2019,
	title = {Inferential {Statistics} as {Descriptive} {Statistics}: {There} {Is} {No} {Replication} {Crisis} if {We} {Don}’t {Expect} {Replication}},
	volume = {73},
	issn = {0003-1305},
	shorttitle = {Inferential {Statistics} as {Descriptive} {Statistics}},
	url = {https://doi.org/10.1080/00031305.2018.1543137},
	doi = {10.1080/00031305.2018.1543137},
	abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
	number = {sup1},
	urldate = {2021-01-26},
	journal = {The American Statistician},
	author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
	month = mar,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1543137},
	keywords = {Auxiliary hypotheses, Confidence interval, Hypothesis test, P-value, Posterior probability, Replication, Selective reporting, Significance test, Statistical model, Unreplicable research},
	pages = {262--270}
}

@article{piperExactReplicationFoundation2019,
	title = {Exact replication: {Foundation} of science or game of chance?},
	volume = {17},
	issn = {1545-7885},
	shorttitle = {Exact replication},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000188},
	doi = {10.1371/journal.pbio.3000188},
	abstract = {The need for replication of initial results has been rediscovered only recently in many fields of research. In preclinical biomedical research, it is common practice to conduct exact replications with the same sample sizes as those used in the initial experiments. Such replication attempts, however, have lower probability of replication than is generally appreciated. Indeed, in the common scenario of an effect just reaching statistical significance, the statistical power of the replication experiment assuming the same effect size is approximately 50\%—in essence, a coin toss. Accordingly, we use the provocative analogy of “replicating” a neuroprotective drug animal study with a coin flip to highlight the need for larger sample sizes in replication experiments. Additionally, we provide detailed background for the probability of obtaining a significant p value in a replication experiment and discuss the variability of p values as well as pitfalls of simple binary significance testing in both initial preclinical experiments and replication studies with small sample sizes. We conclude that power analysis for determining the sample size for a replication study is obligatory within the currently dominant hypothesis testing framework. Moreover, publications should include effect size point estimates and corresponding measures of precision, e.g., confidence intervals, to allow readers to assess the magnitude and direction of reported effects and to potentially combine the results of initial and replication study later through Bayesian or meta-analytic approaches.},
	language = {en},
	number = {4},
	urldate = {2021-01-26},
	journal = {PLOS Biology},
	author = {Piper, Sophie K. and Grittner, Ulrike and Rex, Andre and Riedel, Nico and Fischer, Felix and Nadon, Robert and Siegerink, Bob and Dirnagl, Ulrich},
	month = apr,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Forecasting, Animal studies, Drug therapy, Head, Metaanalysis, Neuroprotectives, Replication studies, Scientists},
	pages = {e3000188}
}

@article{hedgesMoreOneReplication2019,
	title = {More {Than} {One} {Replication} {Study} {Is} {Needed} for {Unambiguous} {Tests} of {Replication}},
	volume = {44},
	issn = {1076-9986},
	url = {https://doi.org/10.3102/1076998619852953},
	doi = {10.3102/1076998619852953},
	abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98\%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
	language = {en},
	number = {5},
	urldate = {2021-01-26},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V. and Schauer, Jacob M.},
	month = oct,
	year = {2019},
	note = {Publisher: American Educational Research Association},
	keywords = {educational policy, evaluation, experimental design, meta-analysis, program evaluation, research methodology, validity/reliability},
	pages = {543--570}
}

@misc{EvaluatingEffectSize,
	title = {Evaluating {Effect} {Size} in {Psychological} {Research}: {Sense} and {Nonsense} - {David} {C}. {Funder}, {Daniel} {J}. {Ozer}, 2019},
	url = {https://journals.sagepub.com/doi/10.1177/2515245919847202},
	urldate = {2021-01-26}
}

@article{harmsBayesFactorReplications2019,
	title = {A {Bayes} {Factor} for {Replications} of {ANOVA} {Results}},
	volume = {73},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2018.1518787},
	doi = {10.1080/00031305.2018.1518787},
	abstract = {With an increasing number of replication studies performed in psychological science, the question of how to evaluate the outcome of a replication attempt deserves careful consideration. Bayesian approaches allow to incorporate uncertainty and prior information into the analysis of the replication attempt by their design. The Replication Bayes factor, introduced by Verhagen and Wagenmakers (2014), provides quantitative, relative evidence in favor or against a successful replication. In previous work by Verhagen and Wagenmakers (2014), it was limited to the case of t-tests. In this article, the Replication Bayes factor is extended to F-tests in multigroup, fixed-effect ANOVA designs. Simulations and examples are presented to facilitate the understanding and to demonstrate the usefulness of this approach. Finally, the Replication Bayes factor is compared to other Bayesian and frequentist approaches and discussed in the context of replication attempts. R code to calculate Replication Bayes factors and to reproduce the examples in the article is available at https://osf.io/jv39h/.},
	number = {4},
	urldate = {2021-01-26},
	journal = {The American Statistician},
	author = {Harms, Christopher},
	month = oct,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2018.1518787},
	keywords = {ANOVA designs, Bayes factor, Replications},
	pages = {327--339}
}

@techreport{muradchanianHowBestQuantify2020,
	title = {How {Best} to {Quantify} {Replication} {Success}? {A} {Simulation} {Study} on the {Comparison} of {Replication} {Success} {Metrics}},
	shorttitle = {How {Best} to {Quantify} {Replication} {Success}?},
	url = {https://osf.io/preprints/metaarxiv/wvdjf/},
	abstract = {To overcome the frequently debated crisis of confidence, replicating studies is becoming increasingly more common. Multiple frequentist and Bayesian measures have been proposed to evaluate whether a replication is successful, but little is known about which method best captures replication success. We studied this in a simulation study, by comparing a number of quantitative measures of replication success with respect to their ability to draw the correct inference when the underlying truth is known, while taking publication bias into account. Our results show that Bayesian metrics seem to slightly outperform frequentist metrics across the board. Generally, meta-analytic approaches seem to slightly outperform metrics that evaluate single studies, except in the scenario of extreme publication bias, where this pattern reverses.},
	urldate = {2021-01-26},
	institution = {MetaArXiv},
	author = {Muradchanian, Jasmine and Hoekstra, Rink and Kiers, Henk and Ravenzwaaij, Don van},
	month = aug,
	year = {2020},
	doi = {10.31222/osf.io/wvdjf},
	note = {type: article},
	keywords = {Economics, Other Social and Behavioral Sciences, Psychology, publication bias, replication success, Social and Behavioral Sciences}
}

@incollection{zondervan-zwijnenburgTestingReplicationSmall2020,
	address = {Abingdon, Oxon ; New York, NY},
	title = {Testing {Replication} with {Small} {Samples}},
	isbn = {978-0-429-27387-2},
	language = {en},
	booktitle = {Small sample size solutions: a guide for applied researchers and practitioners},
	publisher = {Routledge, an imprint of the Taylor \& Francis Group, an informa business},
	author = {Zondervan-Zwijnenburg, Mariëlle and RIjshouwer, Dominique},
	year = {2020},
	keywords = {Data sets, Methodology, Research},
	file = {zondervan-zwijnenburg_rijshouwer_2020_testing_replication_with_small_samples.pdf:/Users/leonreteig/Zotero/storage/NQTJWTIK/zondervan-zwijnenburg_rijshouwer_2020_testing_replication_with_small_samples.pdf:application/pdf}
}

@article{lebelBriefGuideEvaluate2019,
	title = {A {Brief} {Guide} to {Evaluate} {Replications}},
	volume = {3},
	copyright = {Copyright (c) 2019 Etienne Philippe LeBel, Wolf Vanpaemel, Irene Cheung, Lorne Campbell},
	issn = {2003-2714},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/843},
	doi = {10.15626/MP.2018.843},
	language = {en},
	urldate = {2021-01-26},
	journal = {Meta-Psychology},
	author = {LeBel, Etienne Philippe and Vanpaemel, Wolf and Cheung, Irene and Campbell, Lorne},
	month = jun,
	year = {2019},
	keywords = {transparency, reproducibility, direct replication, replicability, evaluating replications}
}

@article{heldNewStandardAnalysis2020,
	title = {A new standard for the analysis and design of replication studies},
	volume = {183},
	copyright = {© 2019 The Authors Journal of the Royal Statistical Society: Series A (Statistics in Society) Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society.},
	issn = {1467-985X},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12493},
	doi = {https://doi.org/10.1111/rssa.12493},
	abstract = {A new standard is proposed for the evidential assessment of replication studies. The approach combines a specific reverse Bayes technique with prior-predictive tail probabilities to define replication success. The method gives rise to a quantitative measure for replication success, called the sceptical p-value. The sceptical p-value integrates traditional significance of both the original and the replication study with a comparison of the respective effect sizes. It incorporates the uncertainty of both the original and the replication effect estimates and reduces to the ordinary p-value of the replication study if the uncertainty of the original effect estimate is ignored. The framework proposed can also be used to determine the power or the required replication sample size to achieve replication success. Numerical calculations highlight the difficulty of achieving replication success if the evidence from the original study is only suggestive. An application to data from the Open Science Collaboration project on the replicability of psychological science illustrates the methodology proposed.},
	language = {en},
	number = {2},
	urldate = {2021-01-26},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Held, Leonhard},
	year = {2020},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12493},
	keywords = {Power, Prior–data conflict, Replication success, Reverse Bayes technique, Sample size, Sceptical p-value},
	pages = {431--448}
}

@techreport{lindeDecisionsEquivalenceComparison2020,
	type = {preprint},
	title = {Decisions {About} {Equivalence}: {A} {Comparison} of {TOST}, {HDI}-{ROPE}, and the {Bayes} {Factor}},
	shorttitle = {Decisions {About} {Equivalence}},
	url = {https://osf.io/bh8vu},
	abstract = {Some important research questions require the ability to ﬁnd evidence for two conditions being practically equivalent. This is impossible to accomplish within the traditional frequentist null hypothesis signiﬁcance testing framework; hence, other methodologies must be utilized. We explain and illustrate three approaches for ﬁnding evidence for equivalence: The frequentist two one-sided tests procedure, the Bayesian highest density interval region of practical equivalence procedure, and the Bayes factor interval null procedure. We compare the classiﬁcation performances of these three approaches for various plausible scenarios. The results indicate that the Bayes factor interval null approach compares favorably to the other two approaches in terms of statistical power. Critically, compared to the Bayes factor interval null procedure, the two one-sided tests and the highest density interval region of practical equivalence procedures have limited discrimination capabilities when the sample size is relatively small: speciﬁcally, in order to be practically useful, these two methods generally require over 250 cases within each condition when rather large equivalence margins of approximately 0.2 or 0.3 are used; for smaller equivalence margins even more cases are required. Because of these results, we recommend that researchers rely more on the Bayes factor interval null approach for quantifying evidence for equivalence, especially for studies that are constrained on sample size.},
	language = {en},
	urldate = {2021-01-26},
	institution = {PsyArXiv},
	author = {Linde, Maximilian and Tendeiro, Jorge and Selker, Ravi and Wagenmakers, Eric-Jan and van Ravenzwaaij, Don},
	month = nov,
	year = {2020},
	doi = {10.31234/osf.io/bh8vu},
	file = {linde_2020_decisions_about_equivalence_-_a_comparison_of_tost,_hdi-rope,_and_the_bayes.pdf:/Users/leonreteig/Zotero/storage/CE49K9IK/linde_2020_decisions_about_equivalence_-_a_comparison_of_tost,_hdi-rope,_and_the_bayes.pdf:application/pdf}
}

@article{lakensImprovingInferencesNull2020,
	title = {Improving {Inferences} {About} {Null} {Effects} {With} {Bayes} {Factors} and {Equivalence} {Tests}},
	volume = {75},
	issn = {1079-5014},
	url = {https://doi.org/10.1093/geronb/gby065},
	doi = {10.1093/geronb/gby065},
	abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.},
	number = {1},
	urldate = {2021-01-26},
	journal = {The Journals of Gerontology: Series B},
	author = {Lakens, Daniël and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
	month = jan,
	year = {2020},
	pages = {45--57}
}

@techreport{leplaaBayesianEvaluationReplication2020,
	title = {Bayesian evaluation of replication studies},
	url = {https://psyarxiv.com/49tbz/},
	abstract = {In this paper a method is proposed to determine whether the result from an original study is corroborated in a replication study. The paper is illustrated using data from the reproducibility project psychology by the Open Science Collaboration. This method emphasizes the need to determine what one wants to replicate: the hypotheses as formulated in the introduction of the original paper, or hypotheses derived from the research results presented in the original paper. The Bayes factor will be used to determine whether the hypotheses evaluated in/resulting from the original study are corroborated by the replication study. Our method to assess the successfulness of replication will better fit the needs and desires of researchers in fields that use replication studies.},
	urldate = {2021-01-26},
	institution = {PsyArXiv},
	author = {Leplaa, Hidde Jelmer and Rietbergen, Charlotte and Hoijtink, Herbert},
	month = may,
	year = {2020},
	doi = {10.31234/osf.io/49tbz},
	note = {type: article},
	keywords = {Meta-science}
}

@article{schauerReconsideringStatisticalMethods2020,
	title = {Reconsidering {Statistical} {Methods} for {Assessing} {Replication}},
	issn = {1082-989X},
	url = {https://www.scholars.northwestern.edu/en/publications/reconsidering-statistical-methods-for-assessing-replication},
	doi = {10.1037/met0000302},
	language = {English (US)},
	urldate = {2021-01-26},
	journal = {Psychological methods},
	author = {Schauer, J. M. and Hedges, L. V.},
	year = {2020},
	note = {Publisher: American Psychological Association Inc.}
}

@article{London2021,
	title = {No {Effect} of {Transcranial} {Direct} {Current} {Stimulation} over {Left} {Dorsolateral} {Prefrontal} {Cortex} on {Temporal} {Attention}},
	volume = {33},
	issn = {0898-929X},
	url = {https://doi.org/10.1162/jocn_a_01679},
	doi = {10.1162/jocn_a_01679},
	abstract = {Selection mechanisms that dynamically gate only relevant perceptual information for further processing and sustained representation in working memory are critical for goal-directed behavior. We examined whether this gating process can be modulated by transcranial direct current stimulation (tDCS) over left dorsolateral prefrontal cortex (lDLPFC)—a region known to play a key role in working memory and conscious access. Specifically, we examined the effects of tDCS on the magnitude of the “attentional blink” (AB), a deficit in identifying the second of two targets presented in rapid succession. Thirty-four participants performed an AB task before (baseline), during and after 20 min of 1-mA anodal and cathodal tDCS in two separate sessions. On the basis of previous reports linking individual differences in AB magnitude to individual differences in DLPFC activity and on the basis of suggestions that effects of tDCS depend on baseline brain activity levels, we hypothesized that anodal tDCS over lDLPFC would modulate the magnitude of the AB as a function of individual baseline AB magnitude. Behavioral results did not provide support for this hypothesis. At the group level, we also did not observe any significant effects of tDCS, and a Bayesian analysis revealed strong evidence that tDCS to lDLPFC did not affect AB performance. Together, these findings do not support the idea that there is an optimal level of prefrontal cortical excitability for cognitive function. More generally, they add to a growing body of work that challenges the idea that the effects of tDCS can be predicted from baseline levels of behavior.},
	number = {4},
	urldate = {2021-03-21},
	journal = {Journal of Cognitive Neuroscience},
	author = {London, Raquel E. and Slagter, Heleen A.},
	month = apr,
	year = {2021},
	pages = {756--768},
	file = {Full Text PDF:/Users/leonreteig/Zotero/storage/HSVLJK4L/London and Slagter - 2021 - No Effect of Transcranial Direct Current Stimulati.pdf:application/pdf;Snapshot:/Users/leonreteig/Zotero/storage/9DSEPPCR/No-Effect-of-Transcranial-Direct-Current.html:text/html}
}
