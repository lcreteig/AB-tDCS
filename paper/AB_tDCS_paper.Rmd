---
title             : "Effects of tDCS on the attentional blink"
shorttitle        : "AB-tDCS"

author:
  - name          : "Leon C. Reteig"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
  - name          : "Lionel A. Newman"
    affiliation   : "1"
  - name          : "K. Richard Ridderinkhof"
    affiliation   : "1"
  - name          : "Heleen A. Slagter"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "UvA"

note: "OUTLINE"

appendix          : "AB_tDCS_appendix.Rmd"
bibliography      : ["AB_tDCS.bib", "r-references.bib"]
link-citations    : true

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf #papaja::apa6_word
---

```{r setup, include = FALSE}
library(papaja) # APA formatted manuscripts
library(here) # (relative) file paths
library(ggm) # partial correlations
library(psychometric) # confidence intervals for (partial) correlations
library(pwr) # power analysis
library(TOSTER) # equivalence tests
library(predictionInterval) # prediction intervals
library(metafor) # meta-analysis
library(afex) # analysis of factorial experiments (repeated measures anova)
library(tidyverse)  # importing, transforming, and visualizing data frames
library(cowplot) # themes and placement of graphs (installed from github)
source(here("src", "func", "behavioral_analysis.R")) # loading data and calculating measures
source(here("src", "lib", "appendixCodeFunctionsJeffreys.R")) # replication Bayes factors

base_font_size <- 7;
geom_text_size <- base_font_size * 0.35 # so geom_text size is same as axis text
base_font_family <- "Helvetica";
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

__Attentional blink phenomenon__ [@Raymond1992; @Dux2009; @Martens2010]

Fundamental bottle-neck? Can it be overcome? -->

__Diminishing the attentional blink__

- Mental training [@Slagter2007]
- Also NIBS [@Dayan2013]:
    - rTMS [@Cooper2004], iTBS [@Esterman2017], cTBS [@Arasanz2012] affect target perception, but not the AB itself (no interactions with lag)
    - tDCS
        - current flow from anode to cathode [@Gebodh2019a]
        - generally opposite effect of anodal and cathodal on cortical excitability [@Nitsche2000] (although this depends on a lot of assumptions [@Bikson2019; @Stagg2018; @Liu2018])

__@London2015 __

- First tDCS study of the AB
    - experimental design
    - tDCS parameters
- No group effects
- However: Opposing effects of anodal and cathodal within an individual

__Aims of present study__

- replicate @London2015 (exploratory analysis)
- extend with EEG, to obtain neurophysiological correlate of individual differences in tDCS response [@Krause2014; @Li2015b; @Harty2017] <!-- Mention this here or only in the methods? -->

__Results preview__

- Like @London2015, no group effect
- However, also no significant correlation in same direction. But this does not say much [@Harms2018; @Simonsohn2015]
- In addition, we aim to maximize the evidential value in these two studies, by using different methods to evaluate whether study 2 is a successful replication of study 1 [@Zwaan2018; @Camerer2018; @OSC2015]
- Reproducibility is a current issue in brain stimulation research [@Heroux2017]; auxiliary goal of this article is to provide a tutorial that might be useful to the brain stimulation community.
<!-- Might be effective to have a target group, but in principle all these methods have nothing to do with brain stimulation in particular -->

# Methods
<!-- We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study [@Simmons2012]. -->

## Participants

```{r load subject info}
subs_incomplete <- c("S03", "S14", "S25", "S29", "S31", "S38", "S43", "S46") # these participants have incomplete data and should be excluded
subject_info <- read_csv2(here("data", "subject_info.csv"), col_names = TRUE, progress = FALSE, col_types = cols(
  first.session = readr::col_factor(c("anodal", "cathodal")),
  gender = readr::col_factor(c("male", "female")),
  age = col_integer())) %>%
  filter(!(subject %in% c(subs_incomplete, "S42"))) %>%
  # recode first.session ("anodal" or "cathodal") to session.order ("anodal first", "cathodal first")
  mutate(first.session = parse_factor(paste(first.session, "first"),
                                      levels = c("anodal first", "cathodal first"))) %>%
  rename(session.order = first.session)
```

```{r demographics}
n_female <- sum(subject_info$gender == "female")
age_mean <- mean(subject_info$age, na.rm = TRUE)
age_sd <- sqrt(sum((subject_info$age - age_mean)^2, na.rm=TRUE)/sum(!is.na(subject_info$age))) # population SD
age_min <- min(subject_info$age, na.rm = TRUE)
age_max <- max(subject_info$age, na.rm = TRUE)
```

48 participants took part in the study. 8 were excluded after session 1:

* 6 because their T1 performance in session 1 was less than 63% correct (i.e. 2 SDs below mean in separate pilot study (n=10)).
* 1 did not show up for the 2nd session.
* 1 because they developed a headache after session 1.

This left 40 participants (`r n_female` female, mean age = `r printnum(age_mean)`, _SD_ = `r printnum(age_sd)`, range = `r age_min`--`r age_max`), which was our target sample size determined before starting data collection.

## Procedure

See Figure \@ref(fig:fig-procedure).

Sessions typically took place exactly one week apart (minimum of 5 days).

`r sum(subject_info$session.order == "anodal first")` participants received anodal tDCS in the first session and cathodal tDCS in the second (vice versa for the remaining `r sum(subject_info$session.order == "cathodal first")` participants)

```{r fig-procedure, fig.cap='(ref:caption-fig-procedure)'}
knitr::include_graphics("figures/figure_1_procedure.png", auto_pdf = TRUE)
```

(ref:caption-fig-procedure) __Procedure__

<!-- Left out sEBR here; should this be disclosed? -->

## Task

See Figure \@ref(fig:fig-task)

```{r fig-task, fig.cap='(ref:caption-fig-task)'}
knitr::include_graphics("figures/figure_2_task.png", auto_pdf = TRUE)
```

(ref:caption-fig-task) __Task__

```{r load study 2 data}
df_study2 <- load_data_study2(here("data"), subs_incomplete) %>%
  filter(complete.cases(.)) %>% # discard rows with data from incomplete subjects
  # recode first.session ("anodal" or "cathodal") to session.order ("anodal first", "cathodal first")
  mutate(first.session = parse_factor(paste(first.session, "first"),
                                       levels = c("anodal first", "cathodal first"))) %>%
  rename(session.order = first.session)
```

```{r trial counts}
trials_study2 <- df_study2 %>%
  group_by(lag) %>%
  summarise_at(vars(trials), funs(mean, min, max, sd))
```

Trial duration varied slightly (because responses were self-paced), so not all participants completed the exact same amount of trials within a block. On average, participants completed `r printnum(trials_study2$mean[trials_study2$lag==3], digits = 0)` short lag trials (_SD_ = `r printnum(trials_study2$sd[trials_study2$lag==3], digits = 0)`; range = `r trials_study2$min[trials_study2$lag==3]`--`r trials_study2$max[trials_study2$lag==3]`) and `r printnum(trials_study2$mean[trials_study2$lag==8], digits = 0)` long lag trials (_SD_ = `r printnum(trials_study2$sd[trials_study2$lag==8], digits = 0)`; range = `r trials_study2$min[trials_study2$lag==8]`--`r trials_study2$max[trials_study2$lag==8]`) per 20-minute block.

## EEG acquisition and preprocessing

## Data analysis
<!-- We used `r cite_r("r-references.bib")` for all our analyses. -->

### Individual differences analysis

### Replication analyses

1. Equivalence tests
2. Prediction interval
3. Replication Bayes factor
4. Include data from study 1
5. Meta-analysis

### Group-level analysis

## Data, materials, and code availability

# Results

## Individual differences

See Figure \@ref(fig:fig-corr).

```{r calculate measures}
ABmag_study2 <- calc_ABmag(df_study2)
ABmagChange_study2 <- calc_change_scores(ABmag_study2)
```

(ref:caption-fig-corr) __Anodal vs. cathodal correlation__

```{r fig-corr, fig.cap='(ref:caption-fig-corr)', fig.width=5.512}

ABlabs <- tibble(x = c(-.14, .5),  y = c(-.44, -.20), label = "Smaller AB", size = 8, angle = c(0,90))

ABmagChange_study2 %>%
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>%

  ggplot(aes(anodal, cathodal)) +
  facet_wrap(~change) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(shape = 21, fill = "gray25", size = 2, color = "white", stroke = .3) +
  geom_rug(colour = "gray25", alpha = .75) +
  scale_x_continuous("Change in anodal session", breaks = seq(-.3,.3,.1), labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous("Change in cathodal session", breaks = seq(-.3,.3,.1), labels = scales::percent_format(accuracy = 1)) +
  coord_equal(xlim = c(-.32, .32), ylim = c(-.32, .32), clip = "off") +
  annotate("segment", 
           x = c(-.05, .05, -.45, -.45), 
           xend = c(-.3, .3, -.45, -.45),
           y = c(-.42, -.42, -.05, .05), 
           yend = c(-.42, -.42, -.3, .3),
          arrow = arrow(angle = 20, length = unit(4,"mm"))) +
  annotate("text", x = -.05, y = -.44, label = "Smaller AB", size = geom_text_size, hjust = "right") +
  annotate("text", x = .05, y = -.44, label = "Larger AB", size = geom_text_size, hjust = "left") +
  annotate("text", x = -.47, y = -.25, label = "Smaller AB", 
           size = geom_text_size, angle = 90, hjust = "left") + 
  annotate("text", x = -.47, y = .25, label = "Larger AB", 
           size = geom_text_size, angle = 90, hjust = "right") +
  theme_minimal_grid(font_size = base_font_size, font_family = base_font_family) +
  theme(strip.background = element_rect(fill = "grey90"), # facet header background
        #panel.spacing.x = unit(2,"lines"), # add horizontal space between facets for annotations
        # move axis labels away from annotatations
        axis.title.x = element_text(margin = margin(t = 15, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0)))
```

```{r partial correlation}
p_r_2 <- pcorr_anodal_cathodal(ABmagChange_study2)
r_study2 <- p_r_2$r # r-value in study 2
n_study2 <- n_distinct(df_study2$subject) # number of participants in study 2
n_study2_pc <- n_study2-1 # one less due to additional variable in partial correlation
```

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order: _r_ = `r printnum(p_r_2$r, gt1=F)`, _t_(`r p_r_2$df`) = `r printnum(p_r_2$tval)`, _p_ = `r printp(p_r_2$pvalue)`

## Is the correlation in study 2 significantly small?

```{r get data from study 1}
df_study1 <- read_tsv(here("data","AB-tDCS_study1.txt"), col_names = TRUE, locale = locale(decimal_mark = ","), cols(
  .default = col_double(),
  First_Session = col_integer(),
  fileno = col_character())) %>% # read study 1 data
  format_study2() # format data from study 1 as in study 2
```

```{r calculate correlation study 1}
ABmag_study1 <- calc_ABmag(df_study1)
ABmagChange_study1 <- calc_change_scores(ABmag_study1)
p_r_1 <- pcorr_anodal_cathodal(ABmagChange_study1)

r_study1 <- p_r_1$r
n_study1 <- n_distinct(df_study1$subject)
n_study1_pc <- n_study1-1 # one less due to additional variable in partial correlation
```

Equivalence tests allow you to test for the _absence_ of an effect of a specific size [@Lakens2018]. Usually this is the smallest effect size of interest (the SESOI). Formally, the test allows you to reject the null hypothesis that the effect size is larger than the SESOI.

Equivalence tests are two one-sided tests: one that the effect exceeds the upper equivalence bound (positive SESOI), and one that the effect exceeds the lower equivalence bound (negative SESOI). However, given that this is a replication study, we have a directional hypothesis: the effects of anodal and cathodal tDCS were negatively correlated in study 1. Because we're only interested in negative effect sizes, we can perform a single one-sided test, also known as an _inferiority test_ [@Lakens2018].

There are many different approaches to setting the SESOI. The most straightforward is to use the correlation in study 1 (_r_ = `r printnum(r_study1, gt1=F)`). However, considering this is only a single study, it is most likely an overestimation of the true effect size. Instead, we'll use the "small telescopes" approach [@Simonsohn2015], which suggest that the SESOI in a replication study should be set to $r_{33\%}$: the effect size the original study had 33% power to detect.

> A replication that obtains an effect size that is statistically significantly smaller than $r_{33\%}$ is inconsistent with the notion that the studied effect is large enough to have been detectable with the original sample size
>
> -- <cite>@Simonsohn2015</cite>

The idea is that an effect smaller than $r_{33\%}$ could not have been properly investigated in the original study in the first place (as the odds were already stacked 2:1 against finding an effect of that size). If the "small telescopes" test is significant, this does not prove the effect doesn't exist at all (as it could be smaller than $r_{33\%}$ still), but it does suggest the effect is so small that study 1 "used too small of a telescope" to reliably detect it.

```{r equivalence test - small telescopes, include=FALSE}
SESOI_small_telescopes <- pwr.r.test(n_study1_pc, power = 0.33) # r value with 33% power in study 1
test_small_telescopes <- TOSTr(n = n_study2_pc,# equivalence test
      r = r_study2,
      low_eqbound_r = -SESOI_small_telescopes$r,
      high_eqbound_r = SESOI_small_telescopes$r,
      alpha = 0.05)
```

In study 1, there was a significant partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order: _r_ = `r printnum(p_r_1$r, gt1=F)`, _t_(`r p_r_1$df`) = `r p_r_1$tval`, _p_ = `r printp(p_r_1$pvalue)`.

Given the sample size of `r n_study1` participants, the correlation study 1 had 33% power to detect is $r_{33\%}$ = `r printnum(SESOI_small_telescopes$r, gt1=F)`.

The correlation in study 2 is significantly less negative than `r printnum(-SESOI_small_telescopes$r,gt1=F)` (_p_ = `r printp(test_small_telescopes$TOST_p1)`) (Figure \@ref(fig:fig-rep-estimate)). The effect in study 2 is therefore "statisically equivalent": the correlation falls within the lower equivalence bound, so it is too small (not negative enough) to be meaningful (note though that it is not equivalent to the upper bound (_p_ = `r printp(test_small_telescopes$TOST_p2)`)).

## Is the correlation in study 2 consistent with study 1?

### Prediction interval

To evaluate whether the result observed in study 2 is consistent with study 1, we can construct a _prediction interval_ (PI) [@Patil2016; @Spence2016]. A prediction interval contains the range of correlation coefficients we can expect to see in study 2, based on the original correlation in study 1, and the sample sizes of both study 1 and 2.

If the original study were replicated 100 times, 95 of the observed correlation coefficients would fall within the 95% prediction interval. Note that this is related to but different from a _confidence interval_ (CI), which quantifies uncertainty about the (unknown) true correlation in the population (95 out of every hundred 95% CIs contain the true population parameter). Because PIs concerns the next single observation, they make a more specific claim, and will therefore be wider than CIs.

```{r prediction interval, include=FALSE}
pred_int <- pi.r(r = r_study1, n = n_study1_pc, rep.n = n_study2_pc, prob.level = 0.95)
pred_int
```

The 95% PI[`r printnum(pred_int$lower_prediction_interval)` -- `r printnum(pred_int$upper_prediction_interval)`] around the original correlation (_r_ = `r printnum(r_study1, gt1=F)`) is very wide, so almost any negative correlation would fall within it. However, it does not include the correlation in study 2 (_r_ = `r printnum(r_study2, gt1=F)`) (Figure \@ref(fig:fig-rep-estimate)). If there was no meaningful difference between study 1 and 2, the correlation observed in study 2 has a 95% chance to fall within the interval (5% due to sampling error). In other words, the correlation in study 2 is inconsistent with study 1, so should not be considered a succesful replication of study 1.

### Replication Bayes Factor

Another approach to quantify the consistency between study 1 and 2 is to construct a replication Bayes Factor.

Instead of the question that the default Bayes factor adresses ("_Is there more evidence that the effect is absent (H~0~) vs. present (H~1~)?_"), we are interested in the question "_Is there more evidence that the effect is absent (H~skeptic~) vs. similar to what was found before (H~proponent~)?_" [@Verhagen2014]. The "replication Bayes factor" adresses this latter question by using the posterior of study 1 as a prior in the analysis of study 2 (i.e. as the proponent's hypothesis). The replication Bayes factor for correlations was proposed by [@Wagenmakers2016].

```{r replication Bayes Factor}
bf0RStudy1 <- 1/repBfR0(nOri = n_study1_pc, rOri = r_study1,
                        nRep = n_study2_pc, rRep = r_study2)
```

This replication Bayes factor expresses that the data are BF~0r~ = `r printnum(bf0RStudy1)` times more likely under H~skeptic~ than under H~proponent~. Using the conventional Bayes factor interpretation scheme [@Wagenmakers2018], this constitutes strong evidence that the effect in study 2 is absent (vs. consistent with study 1).

## Is the effect signifcant when combining study 1 and 2?

The sample size in both study 1 (n = `r n_study1`) and 2 (n = `r n_study2`) is on the lower end. Therefore, we might get a more accurate estimate of the size of the effect when combining both studies.

### Meta-analysis

First, we can get a meta-analytic estimate of the effect size, and see whether that is significant.

We'll specify the meta-analysis as a fixed effects model, as both studies were highly similar and from the same population (same lab, same university student sample). This does mean our inferences are limited to this particular set of two studies, but that's also what we want to know in this case ("how large is the effect when we pool both studies"), not necessarily "how large is the true effect in the population" (which would be a random-effects meta-analysis).

```{r meta-analysis, include = FALSE}
df_meta <- tibble(authors = c("London & Slagter","Reteig et al."), year = c(2015,2018),
                  ni = c(n_study1_pc, n_study2_pc),
                  ri = c(r_study1, r_study2))

# Meta-analysis
res_z <- rma(ri = ri, ni = ni, data = df_meta,
           measure = "ZCOR", method = "FE", # Fisher-z tranform of r values, fixed effects method
           slab = paste(authors, year, sep = ", ")) # add study info to "res" list
res_r <- predict(res_z, transf = transf.ztor) # transform back to r-values
res_z
```

The meta-analytic estimate of the correlation is _r_ = `r printnum(res_r$pred)`, 95%CI[`r printnum(res_r$ci.lb)` -- `r printnum(res_r$ci.ub)`], _p_ = `r printp(res_z$pval)` (Figure \@ref(fig:fig-rep-estimate)). So when combining the estimates of both studies, the overall correlation is no longer significant.

### Pooling the data

We could also pool the data from study 1 and 2 to re-calculate the correlation on the combined sample (n = `r n_study1 + n_study2`).

The main difference is that study 1 presented T2 at lag 2, 4 or 10, whereas study 2 used lags 3 and 8. The long lags should be fairly comparable, as they are both well outside the attentional blink window. T2|T1 performance for both is between 80-90 % at the group level (although performance is a little better for lag 10).

However, there is a big difference at the short lags: group-level performance at lag 3 (study 2) is 10-15 percentage points higher than at lag 2. Therefore, the best bet is to also create a "lag 3" condition in study 1, by imputing lag 2 and lag 4. Luckily the difference from lag 2 to 4 (and 4 to 10) in study 1 looks fairly linear, so this seems a fair approximation of "true" lag 3 performance.

See Figure \@ref(fig:fig-corr-pooled).

```{r pool data from both studies}
ABmagChange_pooled <- df_study1 %>%
  create_lag3_study1() %>% # create lag 3 condition in study 1
  calc_ABmag() %>% # calculate AB mangnitude
  calc_change_scores() %>% # calculate change from baseline
  bind_rows(.,ABmagChange_study2) %>% # combine with study 2
  mutate(study = ifelse(grepl("^pp",subject), "1", "2")) # add a "study" column, based on subject ID formatting
```

```{r fig-corr-pooled, fig.cap='(ref:caption-fig-corr-pooled)', fig.width=3.54331}
ABmagChange_pooled %>%
  # create two columns of AB magnitude change score during tDCS: for anodal and cathodal
  filter(measure == "AB.magnitude", change == "tDCS - baseline") %>%
  select(-baseline) %>%
  spread(stimulation, change.score) %>% { # "{" so we can access data frame below for subsetting

  ggplot(., aes(anodal, cathodal, colour = study, fill = study, shape = study)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_point(size = 2, color = "white", stroke = .3) +
  geom_rug(data = filter(., study == 1), alpha = .75, sides = "tr") +
  geom_rug(data = filter(., study == 2), alpha = .75, sides = "bl") +
  scale_fill_manual(labels = c("study 1", "study 2"), values = c("#E69F00", "#56B4E9")) +
  scale_colour_manual(labels = c("study 1", "study 2"), values = c("#E69F00", "#56B4E9"), guide = FALSE) +
  scale_shape_manual(labels = c("study 1", "study 2"), values = c(21,24)) +
  scale_x_continuous("Change in anodal session", breaks = seq(-.3,.3,.1), labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous("Change in cathodal session", breaks = seq(-.3,.3,.1), labels = scales::percent_format(accuracy = 1)) +
  coord_equal(xlim = c(-.32, .32), ylim = c(-.32, .32), clip = "off") +
  annotate("segment", 
           x = c(-.05, .05, -.43, -.43), 
           xend = c(-.3, .3, -.43, -.43),
           y = c(-.41, -.41, -.05, .05), 
           yend = c(-.41, -.41, -.3, .3),
           arrow = arrow(angle = 20, length = unit(4,"mm"))) +
  annotate("text", x = -.25, y = -.43, label = "Smaller AB", size = geom_text_size, hjust = "left") +
  annotate("text", x = .25, y = -.43, label = "Larger AB", size = geom_text_size, hjust = "right") +
  annotate("text", x = -.45, y = -.25, label = "Smaller AB", 
           size = geom_text_size, angle = 90, hjust = "left") + 
  annotate("text", x = -.45, y = .25, label = "Larger AB", 
           size = geom_text_size, angle = 90, hjust = "right") +  
  theme_minimal_grid(font_size = base_font_size, font_family = base_font_family) +
  theme(legend.title = element_blank(), legend.background = element_blank(), legend.position = "top", legend.direction = "horizontal",
        axis.title.x = element_text(margin = margin(t = 17.5, r = 0, b = 0, l = 0)),
        axis.title.y = element_text(margin = margin(t = 0, r = 15, b = 0, l = 0)))
}
```

(ref:caption-fig-corr-pooled) __Anodal vs. cathodal correlation__ (pooled data)

```{r partial correlation pooled}
p_r_pooled <- pcorr_anodal_cathodal(ABmagChange_pooled)
```

Partial correlation between anodal and cathodal AB magnitude change score (tDCS - baseline), "controlling" for session order: _r_ = `r printnum(p_r_pooled$r, gt1=F)`, _t_(`r p_r_pooled$df`) = `r printnum(p_r_pooled$tval)`, _p_ = `r printp(p_r_pooled$pvalue)` (Figure \@ref(fig:fig-rep-estimate)).

So the correlation across a combination of the samples from study 1 and 2 is also not significant (and smaller than the meta-analytic correlation that included the original correlation in study 1).

```{r replication plot data frame}
rep_plot_df <- bind_cols(tibble::tribble(
  ~analysis,                                     ~estimate,                                    ~source,
  "Study 1",                                     r_study1,                                     "Study 1",
  "Study 2",                                     r_study2,                                     "Study 2",
  "Equivalence test",                            r_study2,                                     "Study 2",
  "Prediction interval",                         r_study1,                                     "Study 1",
  "Meta-analysis",                               res_r$pred,                                   "NA",
  "Pooled data",                                 p_r_pooled$r,                                 "NA"
), tibble::tribble(

  ~int_min,                                      ~int_max,                                     ~int_type,
  CIr(r_study1,n_study1_pc,.95)[1],              CIr(r_study1,n_study1_pc,.95)[2],              "95% CI",
  CIr(r_study2,n_study2_pc,.95)[1],              CIr(r_study2,n_study2_pc,.95)[2],              "95% CI",
  test_small_telescopes$LL_CI_TOST,              test_small_telescopes$UL_CI_TOST,              "90% CI",
  pred_int$lower_prediction_interval,            pred_int$upper_prediction_interval,            "95% PI",
  res_r$ci.lb,                                   res_r$ci.ub,                                   "95% CI",
  CIr(p_r_pooled$r,n_study1+n_study2_pc,.95)[1], CIr(p_r_pooled$r,n_study1+n_study2_pc,.95)[2], "95% CI"
)) %>%
  mutate(
    analysis = factor(analysis, levels = c("Study 1", "Study 2", "Equivalence test", "Prediction interval", "Meta-analysis", "Pooled data")), # reorder levels for plot
    label = paste0("r = ", round(estimate,2), ", ", int_type, "[",round(int_min,2), ", ", round(int_max,2), "]")
  ) # create labels for plot
```

```{r fig-rep-estimate, fig.cap='(ref:caption-fig-rep-estimate)', fig.width=5.512, fig.height=3.42}
ggplot(rep_plot_df, aes(estimate,analysis)) +
  geom_vline(xintercept = c(0,r_study1,r_study2), linetype = "dashed", color = c("black", "#E69F00", "#56B4E9")) +
  geom_errorbarh(aes(y = analysis, xmin = int_min, xmax = int_max), height = .25, color = "gray50") +
  geom_point(shape = 23, aes(fill = source), size = 2) +
  geom_point(x = -SESOI_small_telescopes$r, y = 4, shape = 4, size = 1, color = "gray50") +
  geom_label(x = .45, aes(y = analysis, label = label), hjust = "left", size = geom_text_size, label.size =0) +
  scale_fill_manual(values = c("gray", "#E69F00", "#56B4E9"), guide = FALSE) +
  scale_y_discrete("", limits = rev(levels(rep_plot_df$analysis))) + # reverse order to match levels in data frame
  scale_x_continuous("correlation coefficient", breaks = seq(-1,1,.2)) +
  coord_cartesian(c(-1,1), clip = "off") +
  theme_minimal_vgrid(font_size = base_font_size, font_family = base_font_family) +
  theme(axis.line = element_blank(), axis.ticks = element_blank(), plot.margin = unit(c(1,3,1,1), "lines"))
```

(ref:caption-fig-rep-estimate) __Replication analyses__ (x indicates $r_{33\%}$ = `r printnum(SESOI_small_telescopes$r, gt1=F)`)

## Group-level analysis

See Figure \@ref(fig:fig-group) and Table \@ref(tab:tab-descriptives-T2)

```{r function to make table rows}
T2_desc <- function(df, block_in) {
  df %>% 
    filter(block == block_in) %>%
    mutate(lag = paste("lag", lag)) %>%
    unite(ID, stimulation, session.order) %>%
    group_by(ID, lag) %>%
    summarise(mean_sd = paste0(printnum(mean(T2.given.T1), digits = 3), " (", 
                               printnum(sd(T2.given.T1), digits = 3), ")")) %>%
    spread(ID, mean_sd) %>%
    select(lag, contains("anodal first"), contains("cathodal first"))
}
  
```


```{r tab-descriptives-T2, results='asis'}

# bind tables for each "block" in a list
T2_lst <- map(list("pre","tDCS","post"), T2_desc, df = df_study2)
names(T2_lst) <- c("baseline","tDCS","post")

apa_table(
  T2_lst
  #, align = c("l", rep("r", 4))
  , col.names = c("block", "anodal", "cathodal", "anodal", "cathodal")
  , caption = "Descriptive statistics for T2|T1 performance"
  , note = "Mean (SD)"
  #, added_stub_head = "Variables"
  , col_spanners = list(`First session: anodal (n = 18)` = c(2, 3), `First session: cathodal (n = 22)` = c(4, 5))
)
```


```{r data for plot}
df_plot_T2 <- df_study2 %>%
  group_by(lag,stimulation,block) %>%
  summarise(Mean = mean(T2.given.T1)) %>% # calculate mean per facto
  # add within-subject confidence intervals
  inner_join(wsci(data = df_study2, id = "subject", factors = c("stimulation", "block", "lag"), dv = "T2.given.T1")) %>%
  rename(CI = T2.given.T1) %>%
  add_column(measure = "T2|T1")

df_plot_T1 <- df_study2 %>%
  group_by(lag,stimulation,block) %>%
  summarise(Mean = mean(T1)) %>% # calculate mean per facto
  # add within-subject confidence intervals
  inner_join(wsci(data = df_study2, id = "subject", factors = c("stimulation", "block", "lag"), dv = "T1")) %>%
  rename(CI = T1) %>%
  add_column(measure = "T1")
```

(ref:caption-fig-group) __Group-level data__

```{r fig-group, fig.cap='(ref:caption-fig-group)', fig.width=5.512, fig.height = 3.42}
ggplot(full_join(df_plot_T1,df_plot_T2), aes(lag, Mean, color = stimulation, shape = stimulation, linetype = measure)) +
  facet_wrap(~block) +
  geom_line(aes(group = interaction(measure, stimulation)), size = .75, position = position_dodge(width = 0.4)) +
  geom_linerange(aes(group = interaction(measure, stimulation), ymin = Mean-CI, ymax = Mean+CI), position = position_dodge(width = 0.4), show.legend = FALSE) +
  geom_point(aes(group = interaction(measure, stimulation)), size = 2, position = position_dodge(width = 0.4)) +
  scale_y_continuous("accuracy", limits = c(0,1), breaks = seq(0,1,.1), labels = scales::percent_format(), expand = c(0,0)) +
  scale_color_manual(values = c("#F25F5C", "#4B93B1")) +
  theme_minimal_hgrid(font_size = base_font_size, font_family = base_font_family) +
  theme(strip.background = element_rect(fill = "grey90"), axis.ticks.x = element_line(), axis.ticks.length = unit(.5,"lines"))
```

### T2|T1

* __DV__: `T2.given.T1`: Proportion of trials which T2 was identified correctly, out of the trials in which T1 was idenitified correctly (T2|T1).
* __Within-subject factors__:
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 2 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

See Table \@ref(tab:tab-anova-T2)

```{r rm anova T2|T1, include=FALSE}
aov_T2.given.T1 <- aov_car(T2.given.T1 ~ session.order + Error(subject/(block*stimulation*lag)),
        data = df_study2)
apa_aov_T2.given.T1 <- apa_print(aov_T2.given.T1, mse = FALSE, in_paren = TRUE)
```

```{r tab-anova-T2, results='asis'}
apa_table(apa_aov_T2.given.T1$table, caption = "Repeated measures ANOVA on T2|T1 performance")
```

There was a clear AB effect on average (main effect of Lag, `r apa_aov_T2.given.T1$statistic$lag`).

The hypothesized three-way interaction between Block, Stimulation, and Lag was not significant (`r apa_aov_T2.given.T1$statistic$block_stimulation_lag`). The higher-order interaction with Session Order was (`r apa_aov_T2.given.T1$statistic$session_order_block_stimulation_lag`). However, this effect seems to mainly reflect changes in lag 8 performance, when anodal tDCS was applied in the first session. Because it does not occur at lag 3 performance, and not for both session orders, this is likely not a genuine effect of tDCS on the AB.

Learning effect (stim * session order: 1st session), stronger in first block. Mostly in lag 3 (improvement) but also in lag 8 (decline): so combined a smaller blink. This mostly occured in anodal.first group and was less pronounced in cathodal first (so 3-way interaction is not significant). BASELINE

### T1

* __DV__: `T1`: Proportion of trials in which T1 was identified correctly.
* __Between-subject factor__: `session.order`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS in the first session. Codes for session order.
* __Within-subject factors__:
  1. `block`, 3 levels: Whether data is before (_pre_), during (_tDCS_) or after (_post_) tDCS
  2. `stimulation`, 2 levels: Whether participant received _anodal_ or _cathodal_ tDCS
  3. `lag`, 2 levels: Whether T2 followed T1 after 2 distractors (_lag 3_), or after 7 distractors (_lag 8_).
* __Subject identifier__: `subject` (n = `r n_study2`).

See Table \@ref(tab:tab-anova-T1)

```{r rm anova study 2 T1, include=FALSE}
aov_T1 <- aov_car(T1 ~ session.order + Error(subject/(block*stimulation*lag)),
        data = df_study2)
apa_aov_T1 <- apa_print(aov_T1, mse = FALSE, in_paren = TRUE)
```

```{r tab-anova-T1, results='asis'}
apa_table(apa_aov_T1$table, caption = "Repeated measures ANOVA on T1 performance")
```

T1 performance was also slightly lower on lag 3 than lag 8 (`r apa_aov_T1$statistic$lag`).

There was also a main effect of Block, reflecting that T1 performance decreased _within_ a session (`r apa_aov_T1$statistic$block`). T1 performance also decreased _across_ the sessions (significant interaction between Stimulation and Session Order: `r apa_aov_T1$statistic$session_order_stimulation`).

# Discussion

__Summary__

All measures indicate that the present study is not a successful replication of @London2015:

- Correlation here is not significant, and not in the same direction.
- Correlation here is significantly smaller than @London2015, and than smallest correlation @London2015 could have reasonably detected (equivalence test for lower bound of $r_{33\%}$ is significant).
- The difference between the correlation here and in @London2015 is greater than expected based on sampling error alone (falls outside of 95% prediction interval).
- The data provide strong evidence for the null hypothesis of zero correlation compared to the alternative that the correlation is as in study 1 (replication BF > 10).
- Combining both studies yields a non-significant effect (meta-analysis and pooling the data).

__Potential explanations__

Studies are similar enough such that we consider this a direct replication. Same location, same student population, and almost the same task.

- Most significant difference is that we moved from lag 2/4/10 [@London2015] to lag 3/8. So attentional blink magnitude (performance difference between shortest and longest lag) was on average smaller in study 2. This may have reduced between-subject variability that is essential for an individual differences analysis. Indeed, from Figure \@ref(fig:fig-corr-pooled) it seems that the sample in @London2015 had a larger spread, also in the change scores for AB magnitude.
- Also, we measured EEG here, but not in @London2015.
    - session took longer to complete
    - possible that presence of EEG cap and electrodes affected current flow.

__Group effect__

In agreement with @London2015, we find no effect of tDCS at the group level. So results of both studies do strongly suggest tDCS (with this montage and parameters) does not affect the attentional blink. This is consistent with recent reviews and meta-analyses highlighting there is little evidence that prefrontal tDCS is effective [@Medina2017]; or if so, that its effects appear inconsistent [@Tremblay2014a], are very small [@Dedoncker2016a], or restricted to a small subset of outcome measures and stimulation parameters [@Imburgio2018].

__Interpreting nulls__

Interpreting null results in brain stimulation is difficult [@DeGraaf2018]:

- We cannot be sure there was sufficient current density in lDLPFC [@Kim2014; @Opitz2015] based on our montage. For that you'd need more precise targeting and/or modeling of the current flow [@DeBerker2013; @Karabanov2019].
- Or that cathodal indeed had an opposite effect to anodal, which cannot be assumed [@Bestmann2014; @Bestmann2017].

There is also the possibility we do not find the effect because the studies were not sufficiently powered [@Minarik2016]. Especially for the individual differences analyses, the sample size in both studies is on the low side:

- _Detecting significant correlations_: the required sample size for a medium-sized correlation (r = 0.3) with 80% power is 84. We do do approach this when combining the samples.
- _Estimating the size of a correlation_: to estimate a medium correlation (r = 0.3) within ± 0.15 with 90% confidence: n = 134 [@Schonbrodt2013]. So though our analyses suggest the correlation is small if anything, we cannot accurately estimate how small even with the combined sample.

These are all decisions to be made at the design stage, which can increase the value of a null result [@DeGraaf2018]. However, after the data are in, there are more tools to increase the value of a null result [@Harms2018], especially in the case of a replication study. We hope that our paper succeeded in doing so in the particular case of these two studies for researchers interested in tDCS effects on the attentional blink.

But also, we hope our paper provides inspiration for the brain stimulation field at large. Many speak of a crisis of reproducibility / confidence [@Heroux2017]. This is not necessarily unique to (electrical) brain stimulation [@Baker2015; @OSC2015; @Camerer2018; @Klein2018], but is perhaps aggravated by the fact that the field is so young [@Parkin2015]. Publication bias is one of the primary causes [@Ferguson2012; @Franco2014; @Fanelli2012], which has caused a call to brain stimulation researchers to publish their null results (Research topic in Frontiers: https://www.frontiersin.org/research-topics/5535/non-invasive-brain-stimulation-effects-on-cognition-and-brain-activity-positive-lessons-from-negativ). Publishing null results and making the most of their interpretation is crucial to better the situation.

\newpage

# References
```{r create_r-references}
#r_refs(file = "r-references.bib", pkgs = c("rmarkdown", "bookdown", "knitr", "papaja", "here", "tidyverse", "ggm", "pwr", "TOSTER", "predictionInterval", "metafor", "afex"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
